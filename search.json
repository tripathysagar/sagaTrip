[
  {
    "objectID": "lorapytorch.html",
    "href": "lorapytorch.html",
    "title": "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters",
    "section": "",
    "text": "üìÖ 31/08/2025\nA hands-on guide to Parameter-Efficient Fine-Tuning using Low-Rank Adaptation\nWhat if you could adapt a large neural network to new tasks while only training 7% of its parameters? Low-Rank Adaptation (LoRA) makes this possible by cleverly decomposing weight updates into smaller matrices.\nIn this tutorial, we‚Äôll build LoRA from scratch using PyTorch, demonstrate it on MNIST classification, and show why it‚Äôs revolutionizing how we fine-tune large language models.\nWhat you‚Äôll learn: - The mathematical intuition behind low-rank decomposition - How to implement LoRA adapters in PyTorch - Why LoRA prevents catastrophic forgetting - Practical tips for hyperparameter tuning (rank, alpha) - How to save and load multiple task-specific adapters\nWhen we fine tune the model, we update all the paramets of the weights. Which might lead to catastrophic forgettign and overfitting. By this method we can update only a subset of the parameters. If you are awaare of resnet block, you can think of LoRA weights are main path and the original model is the identity path. By training we eventually learn wrt the new weights added to the model.\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn",
    "crumbs": [
      "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters"
    ]
  },
  {
    "objectID": "lorapytorch.html#rank-of-a-tensor",
    "href": "lorapytorch.html#rank-of-a-tensor",
    "title": "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters",
    "section": "Rank of a tensor:",
    "text": "Rank of a tensor:\nNo of linearly independent rows or columns. It gives of true dimension of the information of the matrix. For LoRA, when we say ‚Äúrank=8‚Äù, we‚Äôre forcing our adaptation to have at most rank 8, meaning it can only capture 8 independent patterns of change.\n\nrank_1 = torch.tensor([[1., 2.], [2., 4.]])\nrank_2 = torch.tensor([[1., 2.], [3., 4.]])\nzero_matrix = torch.zeros(3, 3)\n\nprint(\"Rank 1 matrix:\")\nprint(rank_1)\nprint(f\"Actual rank: {torch.linalg.matrix_rank(rank_1)}\")\n\nprint(\"\\nRank 2 matrix:\")\nprint(rank_2) \nprint(f\"Actual rank: {torch.linalg.matrix_rank(rank_2)}\")\n\nprint(\"\\nZero matrix:\")\nprint(f\"Actual rank: {torch.linalg.matrix_rank(zero_matrix)}\")\n\nRank 1 matrix:\ntensor([[1., 2.],\n        [2., 4.]])\nActual rank: 1\n\nRank 2 matrix:\ntensor([[1., 2.],\n        [3., 4.]])\nActual rank: 2\n\nZero matrix:\nActual rank: 0",
    "crumbs": [
      "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters"
    ]
  },
  {
    "objectID": "lorapytorch.html#lora",
    "href": "lorapytorch.html#lora",
    "title": "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters",
    "section": "LoRA",
    "text": "LoRA\nLet‚Äôs consider a simple case of matrix multiplication.\n\nW = torch.randn(512, 256)\nx = torch.randn(2, 256)\ny = x @ W.T\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Weight shape: {W.shape}\")\nprint(f\"Output shape: {y.shape}\")\n\nInput shape: torch.Size([2, 256])\nWeight shape: torch.Size([512, 256])\nOutput shape: torch.Size([2, 512])\n\n\nLow-rank decomposition means breaking down a large matrix into the product of two smaller matrices. Think of it this way: 1. Original matrix: 512√ó256 (rank could be up to 256) 1. Low-rank decomposition: A(512*8) * B(8*256)\nThe ‚Äúrank‚Äù is the inner dimension (8 in our case).\n\nrank = 8\nA = torch.randn(512, rank)\nB = torch.randn(rank, 256)\nW_decomposed = A @ B\nprint(f\"A shape: {A.shape}\")\nprint(f\"B shape: {B.shape}\")\nprint(f\"W_decomposed shape: {W_decomposed.shape}\")\nprint(f\"Parameters in original W: {W.numel()}\")\nprint(f\"Parameters in A + B: {A.numel() + B.numel()}\")\n\nA shape: torch.Size([512, 8])\nB shape: torch.Size([8, 256])\nW_decomposed shape: torch.Size([512, 256])\nParameters in original W: 131072\nParameters in A + B: 6144\n\n\nNow we need to understand how LoRA uses this decomposition. In LoRA, we don‚Äôt replace the original weight W. Instead, we ADD the low-rank adaptation to it.\ny_lora = x @ (W + W_decomposed).T = x @ W.T + x @ W_decomposed.T = x @ W.T + (x @ B.T) @ A.T\n\ny_lora = x @ (W + W_decomposed).T \nassert y.shape == y_lora.shape\n\n\ny_efficient = x @ W.T + (x @ B.T) @ A.T\nassert y_efficient.shape == y.shape\nassert torch.allclose(y_lora, y_lora)\n\nSo using matrix decompostion we can represet a higher dim matrix with a couple of lower dim matrixs. Which will be used for efficient learning. Useing the lower matrix we can fine tune bigger model in smaller gpu as they wull need smaller space. We can have many different such adapter for different tasks. There is drawback though we need aditional param to keeptack as well as the computation cost of forward pass increases.",
    "crumbs": [
      "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters"
    ]
  },
  {
    "objectID": "lorapytorch.html#lora-using-pytorchs-nn.module",
    "href": "lorapytorch.html#lora-using-pytorchs-nn.module",
    "title": "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters",
    "section": "LoRA using pytorch‚Äôs nn.Module",
    "text": "LoRA using pytorch‚Äôs nn.Module\nOur aim is following 1. Train a simple neural network on MNIST digits 3 and 4 for binary classification 2. Freeze the original model after training 3. Add LoRA adapters to the same model 4. Fine-tune only the LoRA parameters on digits 7 and 8 5. Compare performance - showing that LoRA can adapt the model to new tasks without changing original weights\nThis demonstrates LoRA‚Äôs key benefit: we can reuse a trained model for new tasks by only training a small number of additional parameters, while keeping the original model intact.\n\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Subset\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\nmnist_train = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\nmnist_test = torchvision.datasets.MNIST('./data', train=False, transform=transform)\n\nlen(mnist_train), len(mnist_test)\n\n(60000, 10000)\n\n\nFiltering datasets\n\ndef filter_classes(typ, classes): \n    dataset = mnist_train if typ == 'train' else mnist_test\n    indices = []\n    for i, (_, label) in enumerate(dataset):\n        if label in classes:\n            indices.append(i)\n    \n    # Create new dataset with remapped labels\n    remapped_data = []\n    for i in indices:\n        x, y = dataset[i]\n        new_y = 0 if y == classes[0] else 1\n        remapped_data.append((x, new_y))\n    \n    return remapped_data\n\n\ntrain_34 = filter_classes('train', [3, 4])\ntest_34 = filter_classes('test', [3, 4])\n\nlen(train_34), len(test_34)\n\n(11973, 1992)\n\n\n\n## dataloaders\ndls1 = {\n    'train' : DataLoader(train_34, batch_size=64, shuffle=True),\n    'valid' : DataLoader(test_34 , batch_size=64)\n}\n\n\nthe base model\n\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 64)\n        self.fc2 = nn.Linear(64, 2)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\nmodel1 = Net()\n\n\n\nTrainer loop\n\nclass Trainer:\n    def __init__(self, dls, model, lr=0.01):\n        self.dls = dls\n        self.model = model\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=lr)\n    \n    def train(self, epochs=1):\n        for epoch in range(epochs):\n            # training loop\n            self.model.train()\n            train_loss = 0\n            for batch_idx, (data, target) in enumerate(self.dls['train']):\n                output = self.model(data)\n                loss = F.cross_entropy(output, target)\n                loss.backward()\n                self.optim.step()\n                self.optim.zero_grad()\n                train_loss += loss.item()\n            \n            # Validation inside epoch loop\n            self.model.eval()\n            correct = 0\n\n            with torch.no_grad():\n                valid_loss = 0\n                for data, target in self.dls['valid']:\n                    output = self.model(data)\n                    pred = output.argmax(dim=1)\n                    correct += pred.eq(target).sum().item()\n                    loss = F.cross_entropy(output, target)\n                    valid_loss += loss.item()\n\n                    \n            accuracy = 100. * correct / len(self.dls['valid'].dataset)\n            print(f'Epoch {epoch+1}: Train Loss: {train_loss/len(self.dls[\"train\"]):.4f}, Valid Loss: {valid_loss/len(self.dls[\"valid\"]):.4f} Accuracy: {accuracy:.2f}%')\n\n\nmodel1 = Net()\nt = Trainer(dls=dls1, model=model1, lr=0.01)\nt.train(1)\n\nEpoch 1: Train Loss: 0.0355, Valid Loss: 0.0131 Accuracy: 99.75%\n\n\n\n\nVisualization\n\nimport matplotlib.pyplot as plt\n\ndef visualize_predictions(model, dataloader, num_samples=8):\n    model.eval()\n    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n    axes = axes.flatten()\n    \n    with torch.no_grad():\n        data, targets = next(iter(dataloader))\n        outputs = model(data)\n        predictions = outputs.argmax(dim=1)\n        \n        for i in range(num_samples):\n            img = data[i].squeeze()\n            axes[i].imshow(img, cmap='gray')\n            axes[i].set_title(f'Pred: {predictions[i]}, True: {targets[i]}')\n            axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n\nvisualize_predictions(model1, dls1['valid'])\n\n\n\n\n\n\n\n\nWe trined for 3 and 4 and got around 99% accurecy. Let‚Äôs move on to using LoRA using Model1 to classification of 7 and 8.\n\n\nLoRA classification of 7 and 8\n\ntrain_78 = filter_classes('train', [7, 8])\ntest_78 = filter_classes('test', [7, 8])\n\nlen(train_78), len(test_78)\n\n(12116, 2002)\n\n\n\ndls2 = {\n    'train' : DataLoader(train_78, batch_size=64, shuffle=True),\n    'valid' : DataLoader(test_78, batch_size=64)\n}\n\n\nx, y = next(iter(dls2['train']))\nprint(f\"Batch shape: {x.shape}, Labels shape: {y.shape}\")\nmodel1\n\nBatch shape: torch.Size([64, 1, 28, 28]), Labels shape: torch.Size([64])\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=2, bias=True)\n)\n\n\n\n\nLoRA model for Linear layers\n\nclass LoRALinear(nn.Module):\n    def __init__(self, original_layer, rank=4, alpha=1):\n        super().__init__()\n        self.original_layer = original_layer\n        self.rank = rank\n        self.alpha = alpha\n        \n        # Freeze original layer\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n            \n        # LoRA parameters\n        in_features = original_layer.in_features\n        out_features = original_layer.out_features\n        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n\n    def forward(self, x):\n        #x = x.view(x.size(0), -1)\n        return  self.original_layer(x) + (x @ self.lora_A.T ) @ self.lora_B.T * (self.alpha / self.rank)\n\nLoRA Weight Initialization: Looking at your code:\nself.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)  # Small random values\nself.lora_B = nn.Parameter(torch.zeros(out_features, rank))        # Zeros!\nThe key insight is that at initialization, we want LoRA to have zero effect:\n\nlora_A starts with small random values\nlora_B starts with zeros\nSo lora_A @ lora_B = small_values @ zeros = zeros This means initially: original_output + 0 = original_output\n\nResNet Connection Analogy 1. Original model = identity path (stable, proven features) 1. LoRA adaptation = residual path (learns what‚Äôs missing) 1. Final output = identity + residual\nIf both A and B started random, the initial LoRA output would be: random_A @ random_B = large random values. This would immediately distort the original model‚Äôs good representations, forcing the optimizer to:\n\nFirst ‚Äúundo‚Äù the random noise\nThen learn the actual adaptation Alpha Parameter: controls the ‚Äústrength‚Äù of the LoRA adaptation. In the forward pass:\n\nreturn original_layer(x) + (x @ lora_A.T) @ lora_B.T * (alpha / rank)\nThe alpha/rank scaling serves two purposes:\nScaling independence: change in rank, the adaptation strength stays consistent Learning rate control: Higher alpha = stronger LoRA influence\n\n\nLoRA model\n\nclass NetLoRA(nn.Module):\n    def __init__(self, original_model, rank=4, alpha=1):\n        super().__init__()\n        self.layers = []\n        self.fc1 = LoRALinear(original_model.fc1, rank, alpha)\n        self.fc2 = LoRALinear(original_model.fc2, rank, alpha)\n    \n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nmodel2 = NetLoRA(model1)\nx, _ = next(iter(dls2['train']))\npred = model2(x)\npred.shape\n\ntorch.Size([64, 2])\n\n\n\n# Checking if the module have grad attributes.\nfor name, param in model2.named_parameters():\n    print(f\"{name}: requires_grad={param.requires_grad}\")\n\nfc1.lora_A: requires_grad=True\nfc1.lora_B: requires_grad=True\nfc1.original_layer.weight: requires_grad=False\nfc1.original_layer.bias: requires_grad=False\nfc2.lora_A: requires_grad=True\nfc2.lora_B: requires_grad=True\nfc2.original_layer.weight: requires_grad=False\nfc2.original_layer.bias: requires_grad=False\n\n\n\n#Lets train using dls2 our 7 and 8 datasets with new lora model\nt2 = Trainer(dls=dls2, model=model2, lr=0.01)\nt2.train(5)\n\nEpoch 1: Train Loss: 0.2800, Valid Loss: 0.0527 Accuracy: 98.15%\nEpoch 2: Train Loss: 0.0249, Valid Loss: 0.0290 Accuracy: 98.85%\nEpoch 3: Train Loss: 0.0190, Valid Loss: 0.0457 Accuracy: 98.35%\nEpoch 4: Train Loss: 0.0192, Valid Loss: 0.0383 Accuracy: 98.40%\nEpoch 5: Train Loss: 0.0150, Valid Loss: 0.0295 Accuracy: 98.90%\n\n\n\n# visualizing the model wrt the valid set\nvisualize_predictions(model2, dls2['valid'])\n\n\n\n\n\n\n\n\n\noriginal_params = sum(p.numel() for p in model1.parameters())\nlora_params = sum(p.numel() for p in model2.parameters() if p.requires_grad)\nprint(f\"Original model: {original_params} parameters\")\nprint(f\"LoRA adapters: {lora_params} parameters\")\nprint(f\"Efficiency: {lora_params/original_params*100:.2f}% of original\")\n\nOriginal model: 50370 parameters\nLoRA adapters: 3656 parameters\nEfficiency: 7.26% of original\n\n\n\n\nSaving LoRA weights and parameters\nTo save LoRA model that we specially trained. We have to save the following : 1. We need to save the hyperparameters (rank, alpha) with the LoRA weights 1. We should filter to save only requires_grad=True parameters 1. This approach allows us to have multiple LoRA adapters for different tasks By doing the above we can save multiple LoRAs for a given base model trained for differeent task. Where each LoRA file would contain both weights and hyperparameters.\n\n# saving base model\nbase_model_pth = \"main_model.pth\"\ntorch.save(model1.state_dict(), base_model_pth)\n!file {base_model_pth}\n\nmain_model.pth: Zip archive data, at least v0.0 to extract, compression method=store\n\n\n\n# saving lora weights \nrank, alpha = 4, 1.\nlora_state = {\n        'rank': rank,\n        'alpha': alpha,\n        'weights': {}\n    }\n    \nfor name, param in model2.named_parameters():\n    # we are only filtering out the lora params whih are added\n    if 'lora' in name and param.requires_grad: \n        lora_state['weights'][name] = param.data\n\nlora_model_pth = 'lora.pth'\ntorch.save(lora_state, lora_model_pth )\n!file {lora_model_pth}\n\nlora.pth: Zip archive data, at least v0.0 to extract, compression method=store\n\n\nLoading the LoRA model back from file\n\n# load base model\nbase_model = Net()\nbase_model.load_state_dict(torch.load(base_model_pth))\nbase_model\n\nNet(\n  (fc1): Linear(in_features=784, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=2, bias=True)\n)\n\n\n\nvisualize_predictions(base_model, dls1['valid'])\n\n\n\n\n\n\n\n\n\n# loading lora params\nlora_data = torch.load(lora_model_pth)\nrank = lora_data['rank']\nalpha = lora_data['alpha']\nrank, alpha\n\n(4, 1.0)\n\n\n\n# Create LoRA model from base model\nlora_model = NetLoRA(base_model, rank=rank, alpha=alpha)\n\n# Then load the LoRA weights\nfor name, param in lora_model.named_parameters():\n    if 'lora' in name and name in lora_data['weights']:\n        param.data.copy_(lora_data['weights'][name])\n\n\n# Load LoRA weights properly\nfor name, param in lora_model.named_parameters():\n    if 'lora' in name and name in lora_data['weights']:\n        param.data.copy_(lora_data['weights'][name])\n\n\n# Test on 7&8 dataset\nvisualize_predictions(lora_model, dls2['valid'])\n\n\n\n\n\n\n\n\n\n\nHyperparameter analysis\n\nanalysis of rank\n\ndef test_rank(rank, alpha=1, epochs=3):\n    print(f\"\\n=== Testing Rank {rank=} and {alpha=} ===\")\n    \n    # Create LoRA model with specific rank\n    lora_model = NetLoRA(model1, rank=rank, alpha=alpha)\n    \n    # Count parameters\n    lora_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n    original_params = sum(p.numel() for p in model1.parameters())\n    efficiency = lora_params/original_params*100\n    \n    print(f\"LoRA parameters: {lora_params}\")\n    print(f\"Efficiency: {efficiency:.2f}% of original\")\n    \n    # Train and get final accuracy\n    trainer = Trainer(dls=dls2, model=lora_model, lr=0.01)\n    trainer.train(epochs)\n    \n    return lora_params, efficiency\n\n# Test different ranks\nranks_to_test = [2, 4, 8, 16]\nresults = []\n\nfor rank in ranks_to_test:\n    params, eff = test_rank(rank)\n    results.append((rank, params, eff))\n\n\n=== Testing Rank rank=2 and alpha=1 ===\nLoRA parameters: 1828\nEfficiency: 3.63% of original\nEpoch 1: Train Loss: 0.3000, Valid Loss: 0.0417 Accuracy: 98.55%\nEpoch 2: Train Loss: 0.0242, Valid Loss: 0.0296 Accuracy: 99.00%\nEpoch 3: Train Loss: 0.0212, Valid Loss: 0.0318 Accuracy: 98.80%\n\n=== Testing Rank rank=4 and alpha=1 ===\nLoRA parameters: 3656\nEfficiency: 7.26% of original\nEpoch 1: Train Loss: 0.2954, Valid Loss: 0.0395 Accuracy: 98.65%\nEpoch 2: Train Loss: 0.0247, Valid Loss: 0.0344 Accuracy: 98.55%\nEpoch 3: Train Loss: 0.0213, Valid Loss: 0.0321 Accuracy: 98.85%\n\n=== Testing Rank rank=8 and alpha=1 ===\nLoRA parameters: 7312\nEfficiency: 14.52% of original\nEpoch 1: Train Loss: 0.2932, Valid Loss: 0.0576 Accuracy: 98.15%\nEpoch 2: Train Loss: 0.0240, Valid Loss: 0.0306 Accuracy: 98.80%\nEpoch 3: Train Loss: 0.0201, Valid Loss: 0.0273 Accuracy: 98.75%\n\n=== Testing Rank rank=16 and alpha=1 ===\nLoRA parameters: 14624\nEfficiency: 29.03% of original\nEpoch 1: Train Loss: 0.2780, Valid Loss: 0.0365 Accuracy: 98.50%\nEpoch 2: Train Loss: 0.0271, Valid Loss: 0.0301 Accuracy: 98.65%\nEpoch 3: Train Loss: 0.0197, Valid Loss: 0.0251 Accuracy: 99.05%\n\n\n\n\nanalysis of alpha\n\nalpahs_to_test = [4, 2, 1, 0.5]\nfor alpha in alpahs_to_test:\n    params, eff = test_rank(rank=4, alpha= alpha)\n    results.append((rank, params, eff))\n\n\n=== Testing Rank rank=4 and alpha=4 ===\nLoRA parameters: 3656\nEfficiency: 7.26% of original\nEpoch 1: Train Loss: 0.1721, Valid Loss: 0.0368 Accuracy: 98.45%\nEpoch 2: Train Loss: 0.0240, Valid Loss: 0.0269 Accuracy: 98.75%\nEpoch 3: Train Loss: 0.0191, Valid Loss: 0.0328 Accuracy: 98.95%\n\n=== Testing Rank rank=4 and alpha=2 ===\nLoRA parameters: 3656\nEfficiency: 7.26% of original\nEpoch 1: Train Loss: 0.2189, Valid Loss: 0.0422 Accuracy: 98.65%\nEpoch 2: Train Loss: 0.0244, Valid Loss: 0.0237 Accuracy: 99.05%\nEpoch 3: Train Loss: 0.0206, Valid Loss: 0.0277 Accuracy: 98.70%\n\n=== Testing Rank rank=4 and alpha=1 ===\nLoRA parameters: 3656\nEfficiency: 7.26% of original\nEpoch 1: Train Loss: 0.2877, Valid Loss: 0.0422 Accuracy: 98.50%\nEpoch 2: Train Loss: 0.0235, Valid Loss: 0.0306 Accuracy: 98.75%\nEpoch 3: Train Loss: 0.0209, Valid Loss: 0.0376 Accuracy: 98.55%\n\n=== Testing Rank rank=4 and alpha=0.5 ===\nLoRA parameters: 3656\nEfficiency: 7.26% of original\nEpoch 1: Train Loss: 0.3995, Valid Loss: 0.0719 Accuracy: 97.65%\nEpoch 2: Train Loss: 0.0307, Valid Loss: 0.0339 Accuracy: 98.65%\nEpoch 3: Train Loss: 0.0230, Valid Loss: 0.0277 Accuracy: 99.00%\n\n\nRank Selection:\n\nStart small: Begin with rank=4 or 8 for most tasks\nRule of thumb: Higher rank = more expressiveness but more parameters\nTask complexity matters:\n\nSimple tasks (like your digit classification): rank=4-8\nComplex tasks (large language models): rank=16-64\n\nDiminishing returns: Performance often plateaus after a certain rank\n\nAlpha Selection:\n\nCommon values: 1, 8, 16, 32 (often powers of 2)\nHigher alpha: Stronger LoRA influence, faster adaptation\nLower alpha: More conservative, slower learning\nStarting point: Try alpha = rank (so alpha=8 for rank=8)\n\nPractical approach:\n\nFix alpha=1, try ranks [4, 8, 16]\nPick best performing rank\nThen tune alpha [0.1, 1, 8, 16] with that rank",
    "crumbs": [
      "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters"
    ]
  },
  {
    "objectID": "lorapytorch.html#conclusion",
    "href": "lorapytorch.html#conclusion",
    "title": "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters",
    "section": "Conclusion",
    "text": "Conclusion\nThrough this hands-on exploration, we‚Äôve demonstrated LoRA‚Äôs core value proposition: achieving strong performance on new tasks while using only a fraction of the original model‚Äôs parameters.\nKey takeaways:\n\nLoRA adapters used only 7.26% of the original parameters yet achieved 98.15% accuracy on a completely different classification task\nThe original model weights remain frozen and unchanged, preventing catastrophic forgetting\nMultiple task-specific LoRA adapters can be saved and swapped for the same base model Why LoRA matters:\nMemory efficient: Fine-tune large models on consumer GPUs\nStorage efficient: Store multiple task adapters instead of full model copies\nModular: Easy to experiment with different tasks without retraining from scratch This simple MNIST example scales to modern LLMs where LoRA enables fine-tuning billion-parameter models with minimal computational resources, making personalized AI more accessible.",
    "crumbs": [
      "LoRA Explained: Fine-Tune Large Models with 90% Fewer Parameters"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html",
    "href": "pythonwalkthrough.html",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "",
    "text": "üìÖ 02/10/2025\nIf you‚Äôre an experienced programmer looking to add Python to your toolkit, this guide is for you(writen for my wife who is a JS devü§´üòÖ). We‚Äôre skipping the ‚Äúwhat is a variable‚Äù explanations and focusing on what makes Python different and powerful.\nThis comprehensive guide covers the essentials you need to be productive in Python: from basic data types and control flow, through lists and dictionaries, to functions, classes, and exception handling. We‚Äôll also touch on Python-specific features like comprehensions and special methods that make the language elegant and expressive.\nLet‚Äôs get started. To start execute in collab execeute a cell by using key (shift and enter or return).",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#primitive-data-types",
    "href": "pythonwalkthrough.html#primitive-data-types",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "Primitive Data Types",
    "text": "Primitive Data Types\n\nint: Whole numbers (e.g., 10, -5, 0)\nfloat: Decimal numbers (e.g., 10.0, 3.14, -2.5)\nstr: Text/strings (e.g., ‚Äúhello‚Äù, ‚Äòworld‚Äô)\nbool: True/False values\nNoneType: Python‚Äôs null value (None)\n\nUsing function type, we can fetch the type of a variable.\n\nnoi = 10\nnof = 10.1\ns = \"i am string\"\nbool_d = True\nnone_d = None\n\n\ntype(noi), type(nof), type(s), type(bool_d), type(none_d)\n\n(int, float, str, bool, NoneType)",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#control-flows",
    "href": "pythonwalkthrough.html#control-flows",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "Control flows",
    "text": "Control flows\n\nIndentation is mandatory - Python uses whitespace to define code blocks, not just for readability\nConsistent indentation - All lines at the same block level must have the same indentation\nStandard is 4 spaces (though tabs work, mixing them causes errors)\nColon usage - Control structures end with a colon : before the indented block\n\n\nConditional statements\n\nage = 18\nif age &gt;= 18:\n    print(\"Adult\")\nelif age &gt;= 13:\n    print(\"Teenager\")\nelse:\n    print(\"Child\")\n\nAdult\n\n\n\n\nLoops\n\n# For loop\nfor i in range(3):\n    print(f\"For: {i=}\")\n\n# While loop\ncount = 0\nwhile count &lt; 3:\n    print(f\"While: {count=}\")\n    count += 1\n\nFor: i=0\nFor: i=1\nFor: i=2\nWhile: count=0\nWhile: count=1\nWhile: count=2",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#basic-data-struct",
    "href": "pythonwalkthrough.html#basic-data-struct",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "Basic Data Struct",
    "text": "Basic Data Struct\nList: Mutable, ordered collection - Can contain different data types - Supports indexing and slicing - Dynamic sizing (can grow/shrink) - use list keyword for init\nDictionary: Mutable, unordered key-value mapping - Keys must be immutable and unique - Fast key-based lookup - Dynamic sizing - use dict keyword for init\n\nList\n\nlis = list([1, 2, 3, 4])\nprint(f\"{lis=}\")\nprint(f\"lenght of the lis : {len(lis)}\")\n# indexing starting for 0\nprint(f\"{lis[0]=}\")\n\nlis=[1, 2, 3, 4]\nlenght of the lis : 4\nlis[0]=1\n\n\n\n# adding new element to the list\nlis.append(5)\nprint(f\"{lis=}\")\n\nlis=[1, 2, 3, 4, 5]\n\n\n\nlis2 = [6, 7]\nlis1 = lis + lis2\nprint(f\"{lis1=}\")\n\nlis1=[1, 2, 3, 4, 5, 6, 7]\n\n\n\n# Get a portion of the list\nprint(f\"{lis1[1:4]=}\")  # elements from index 1 to 3\n\nlis1[1:4]=[2, 3, 4]\n\n\n\nprint(f\"{lis=}\")\nlis.insert(0, 99) # inserting value at the perticular index\nprint(f\"{lis=}\")\n\nlis=[1, 2, 3, 4, 5]\nlis=[99, 1, 2, 3, 4, 5]\n\n\n\nprint(f\"{lis=}\")\nprint(lis.pop()) # remove last element\nprint(f\"{lis=}\")\n\nlis=[99, 1, 2, 3, 4, 5]\n5\nlis=[99, 1, 2, 3, 4]\n\n\n\nprint(f\"{lis=}\")\nprint(lis.remove(2)) # remove the 2nd element\nprint(f\"{lis=}\")\n\nlis=[99, 1, 2, 3, 4]\nNone\nlis=[99, 1, 3, 4]\n\n\n\n\nDictionary\n\ndic = dict({'a':1, 'b':2})\ndic\n\n{'a': 1, 'b': 2}\n\n\n\ndic = dict({'a':1, 'b':2}) # can be written as like {'a':1, 'b':2} without dict\nprint(f\"{dic=}\")\nprint(f\"{dic['a']=}\")     # indexing the dictionary with a key value\n\ndic={'a': 1, 'b': 2}\ndic['a']=1\n\n\n\n# adding new entry to the dictionay\ndic['c'] = 3\nprint(f\"{dic=}\")\n\ndic={'a': 1, 'b': 2, 'c': 3}\n\n\n\nprint(f\"{dic=}\")\nprint(f\"{dic.pop('c')=}\")\nprint(f\"{dic=}\")\n\ndic={'a': 1, 'b': 2, 'c': 3}\ndic.pop('c')=3\ndic={'a': 1, 'b': 2}\n\n\nIterating wrt the Dictionary keys\n\nfor k in dic.keys():\n    print(f\"{k=} -&gt; {dic[k]=}\")\n\nk='a' -&gt; dic[k]=1\nk='b' -&gt; dic[k]=2\n\n\nJust fetching the values in the Dictionary\n\nfor v in dic.values():\n    print(f\"{v=}\")\n\nv=1\nv=2\n\n\niterating wrt both key and values without explicitly indexing\n\nfor k, v in dic.items():\n    print(f\"{k=} -&gt; {v=}\")\n\nk='a' -&gt; v=1\nk='b' -&gt; v=2\n\n\n\n#Safe key access uncommnet the below line and run\n#print(dic['c'])\ndic.get('item', \"does not exists\")\n\n'does not exists'\n\n\n\n# update a perticular value of a given key\ndic['a'] = 1000\nprint(f\"{dic=}\")\n\ndic={'a': 1000, 'b': 2}\n\n\nThere are couple of other Data struct below are those: - Tuple: immutable list,ref - Set: As name suggest it will store object, ref",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#list-and-dictionary-comprehension",
    "href": "pythonwalkthrough.html#list-and-dictionary-comprehension",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "List and Dictionary comprehension",
    "text": "List and Dictionary comprehension\nIt is more consise way to build list and dict with explicitly using those key words. First I will create a list from 0 to 10, then filter out only the positive number.\n\nlis = list(range(10))\nlis\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nodd = [i for i in lis if i % 2]\neven = [i for i in lis if not i % 2 ]\neven, odd\n\n([0, 2, 4, 6, 8], [1, 3, 5, 7, 9])\n\n\n\ndic = {chr(65 + i): i for i in range(5)}\ndic\n\n{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#function",
    "href": "pythonwalkthrough.html#function",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "Function",
    "text": "Function\n\nstarts with key word def\nit should have a have and a list of args\nwe can add in types in the function definations, it will be used for type hinting. The compile don‚Äôt enforce types at runtime. The function accepts any types that support the + operator, even though we hinted int\n\n\ndef func():\n    print(\"Hello world\")\nfunc()\n\nHello world\n\n\n\ndef add(a:int, b:int=0):\n    return a+b\nprint(add(10, 1))\nprint(add(10))          # here b is defults to 0\nprint(add(10.5, 0))\nprint(add('10.5', '0'))\n\n11\n10\n10.5\n10.50\n\n\nA gotcha By defult the function returns None. One should be keep those in mind.\n\ndef show(x):\n    print(f\"{x=}\")\n\nresult = show(10)\nprint(f\"{result=}\")\n\nx=10\nresult=None",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#class",
    "href": "pythonwalkthrough.html#class",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "Class",
    "text": "Class\n\nUsed for bundling objects and functions\nCreating new objects creates a new instance of the class\nEach function have should have a reference to itself usally repesented by self\nHelper functions wrapped in __ that need to be followed for special functionality below are few and respective usages\n\n\n\n\n\n\n\nname\nfunctionality\n\n\n\n\n__init__\nConstructor called when creating a new instance, initializes attributes\n\n\n__repr__ and __str__\nString representation of a class object (repr for developers, str for end users)\n\n\n__iter__\nReturns an iterator object, makes the class iterable\n\n\n__next__\nFetches the next item from the iterator, raises StopIteration when done\n\n\n__getattr__\nCalled when accessing an attribute that doesn‚Äôt exist\n\n\n__getitem__\nEnables indexing and slicing (e.g., obj[key])\n\n\n__setattr__\nCalled when setting an attribute (e.g., obj.attr = value)\n\n\n__setitem__\nEnables item assignment (e.g., obj[key] = value)\n\n\n__del__\nDestructor called when object is about to be destroyed\n\n\n__new__\nCreates and returns a new instance before __init__ is called\n\n\n__enter__\nCalled when entering a context manager (with statement)\n\n\n__exit__\nCalled when exiting a context manager, handles cleanup\n\n\n\nComplete docs are present in docs\nClasses also suppourt inheritace, a base example can be found here\n\n\nclass L:\n    def __init__(self, lis):\n        print(f\"init is called\")\n        self.lis = lis\n    \n    def __str__(self):\n        # it return length of the lis along with first 5 element\n        return f\"{len(self.lis)} {self.lis[:5]}\"\n    \n    def __len__(self):\n        return len(self.lis)\n\nli = L(list(range(10)))\nprint(li)\nprint(f\"{len(li)=}\")\n\ninit is called\n10 [0, 1, 2, 3, 4]\nlen(li)=10\n\n\nMonkey patching is dynamically modifying a class or module at runtime by adding, replacing, or modifying its attributes or methods. While powerful, it should be used cautiously as it can make code harder to understand and maintain.\n\ndef iter(self):\n    for i in self.lis:\n        yield i\n\n# monkey patching \nL.__iter__ = iter\n\nfor i in li:\n    print(i, end=\" \")\n\n0 1 2 3 4 5 6 7 8 9 \n\n\n\ndef get_item(self, key):\n    return self.lis[key]\n\ndef set_item(self, key, val):\n    self.lis[key] = val\n\n\n# monkey patching \nL.__getitem__ = get_item\nL.__setitem__ = set_item\n\nli[0] = -100\nprint(f\"{li[0]=}\")\nstr(li)\n\nli[0]=-100\n\n\n'10 [-100, 1, 2, 3, 4]'",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#exception-handling",
    "href": "pythonwalkthrough.html#exception-handling",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "Exception Handling",
    "text": "Exception Handling\n\nPurpose: Gracefully handle errors instead of crashing the program\ntry block: Contains code that might raise an exception\nexcept block: Catches and handles specific exceptions\nMultiple except blocks: Can catch different exception types separately\nException as e: Captures the exception object for inspection\nfinally block (optional): Always executes, regardless of exceptions (useful for cleanup like closing files)\nRaising exceptions: Use raise to throw exceptions intentionally\nCommon built-in exceptions: ValueError, TypeError, KeyError, IndexError, FileNotFoundError, ZeroDivisionError\n\n\ntry:\n    result = 10 / 0  # This will raise ZeroDivisionError\n    print(f\"{result=}\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\nfinally:\n    print(\"This always runs, error or not\")\n\nCannot divide by zero!\nThis always runs, error or not",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "pythonwalkthrough.html#conclusion",
    "href": "pythonwalkthrough.html#conclusion",
    "title": "Python for Programmers: Fast Track to Productivity",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe have covered some basic part of Python programming language\nThe language is vast there is many other stuff are not covered below are few other imp for reference\n\nFile I/O : Reading and writing files (especially the with statement for context managers)\nLambda function : Anonymous functions for quick operations\nDecorators : A Python-specific feature that‚Äôs commonly used in frameworks\nGenerators : generator for effienct way to iterate\n\nThere are many awsome stuff which I have not included, hope this blog acts as a launchpad for your python learning journey",
    "crumbs": [
      "Python for Programmers: Fast Track to Productivity"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Lets learn deeplearning. This is my blog.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#standalone-learning",
    "href": "index.html#standalone-learning",
    "title": "Home",
    "section": "Standalone Learning",
    "text": "Standalone Learning\n\n\n\nüìÖ Date\nüéØ Blog Post\nüöÄ Run in Colab\n\n\n\n\n31/08/2025\nLLM Text Generation\n\n\n\n31/08/2025\nLoRA PyTorch\n\n\n\n02/10/2025\nPython Walk Through\n\n\n\n15/10/2025\nBuilding a Text Only Nano-GPT from Scratch\n\n\n\n17/10/2025\nBuilding a Multi Modal Nano-GPT from Scratch",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#bigger-experiments",
    "href": "index.html#bigger-experiments",
    "title": "Home",
    "section": "Bigger Experiments",
    "text": "Bigger Experiments\n\n\n\nüìÖ Date\nüéØ Blog Post\n\n\n\n\n23/09/2025\nDomain Adaptation Fine Tune on Teleco Data",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "daftsft.html",
    "href": "daftsft.html",
    "title": "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1",
    "section": "",
    "text": "üìÖ 23/09/2025\nAim of the experiment is to implement train a small LLM on new domain and prime it for that domain specific sythetic data.\nLLM training haapens in two steps: 1. base model : trained on raw text for next token prediction(classic language modeling). Which is done by all language model by training on all the crawed web data. 1. instruct model : Train the base model on the further curated data where real magic happens. Like follwing instruction, multilayer chat, to make it helpful, safe, and aligned with user expectations etc . This process is known as RLHF.\nBelow is a basic pipeline for RLHF, taken from this excellent blog by Chip Huyen",
    "crumbs": [
      "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1"
    ]
  },
  {
    "objectID": "daftsft.html#motivation",
    "href": "daftsft.html#motivation",
    "title": "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1",
    "section": "Motivation",
    "text": "Motivation\nI work at Amdocs in the telecom domain, where a lot of knowledge is stored in Confluence pages. I wanted to explore whether a small model fine-tuned on a subset of our internal wiki could assist with internal Q&A.",
    "crumbs": [
      "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1"
    ]
  },
  {
    "objectID": "daftsft.html#experiment-setup-constraints",
    "href": "daftsft.html#experiment-setup-constraints",
    "title": "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1",
    "section": "Experiment Setup & Constraints",
    "text": "Experiment Setup & Constraints\n\nHardware: Mac M1 (no NVIDIA GPU, so training is on CPU/MPS backend ‚Äî slower than CUDA).\nData: Small subset of internal wiki pages. It belongs to telecom domain written in english, so no need to extend the vocabulary of the tokenizer of the model.\nCompute: Due to hardware constraints, we trained on a very small sample to validate pipeline, not to reach SOTA results.\nGoal:\n\nValidate that DoRA + LoRA SFT works end-to-end.\nMeasure how much domain knowledge the model can absorb with few steps.\n\nCode: All the code for the experiment is present in the qa_sys. Follow the notebook only no propritary data is shared.",
    "crumbs": [
      "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1"
    ]
  },
  {
    "objectID": "daftsft.html#approach",
    "href": "daftsft.html#approach",
    "title": "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1",
    "section": "Approach",
    "text": "Approach\n\nüï∑Ô∏è Data scraping from confluence\nThe data is stored in org‚Äôs private server. I extracted all the pages and all its child wikis using the awesome lib atlassian-python-api. It provides which provides a simple way to connect to Confluence using access token and conflunece url.\nThen, I wrote a small recursive crawler (BFS style) to: 1. Fetch a page‚Äôs content. 1. Retrieve its child pages. 1. Repeat until the entire hierarchy is traversed.\nFor each page I used html2text lib for converting a simple webpage to markdown dump as well as few clean ups like comments attributes for simplicity. Fetched around 4654 pages.\nThis gives a structural raw text to be used for training base model.\n\n\nüèóÔ∏è Building Synthetic Data\n\nSynthetic Data Kit : I used Meta‚Äôs synthetic-data kit to generate question‚Äìanswer pairs from each page.\nRole-based Diversity : To make the dataset more robust, I instructed the model to generate Q&A pairs from the perspective of four roles:\n\nBA (Business Analyst) ‚Äì business rules, compliance, ROI.\nSA (System Analyst) ‚Äì workflows, dependencies, data flows.\nDEV (Developer) ‚Äì API inputs/outputs, error handling, edge cases.\nQA (Tester) ‚Äì test cases, edge cases, validation.\n\nThe SDK was configured to use openai/gpt-oss-20b, running locally via LM Studio on my Mac M1.\nFocus on Speed :\n\nI only ran the create step of the SDK (no curation) to minimize generation time on Mac hardware.\nThis meant I directly collected the generated Q&A without additional filtering.\n\nPrompt : Below is the exact prompt I used to generate Q&A pairs using SDK: ```yml qa_generation: | You are a synthetic data generator for API and business process documentation. The input is a document describing one or more processes, APIs, or business requirements.\nInstructions:\n\nAutomatically identify the document title from the content.\n- Use the inferred title naturally in every question and answer.\n- If no clear title exists, you may use the filename without extension as the title.\n\nGenerate question-and-answer pairs for the following roles:\n- Business Analyst (BA): focus on requirements, stakeholder value, business rules, process optimization, compliance, risk, and ROI.\n- System Analyst / Software Analyst (SA): focus on system interactions, workflow, dependencies, data flows, and integration points.\n- Developer (DEV): focus on inputs, outputs, API parameters, request/response examples, error handling, and implementation considerations.\n- QA / Tester (QA): focus on test cases, edge cases, validation, error scenarios, and business rule verification.\nEach question and answer must refer to the inferred title naturally. Example: - Question: ‚ÄúFor the User Management process, what are the steps and required inputs?‚Äù - Answer: ‚ÄúThe User Management process requires two APIs: GET token for authentication and PUT ManageUser for updating user details.‚Äù\nOnly generate Q&A from the provided content; do not invent information.\nIf the document contains only links or very little meaningful content, output an empty array [].\nEnsure questions and answers are clear, self-contained, and unambiguous.\nAvoid duplicate questions.\nGenerate at least one Q&A per role if information allows.\nEach question and answer must integrate the inferred title naturally.\nDo not infer or invent details not present in the document. If unclear, omit the Q&A for that role.\nIf the document is processed in chunks, ensure Q&A is relevant only to the current chunk.\nExtract all high qualities Q&A pairs possible, up to the 5 per role.\nThink hard before generation\n\n **Output format:**\n   [\n     {{\n       \"title\": \"inferred_document_title\",\n       \"role\": \"BA\",\n       \"question\": \"...\",\n       \"answer\": \"...\"\n     }},\n     {{\n       \"title\": \"inferred_document_title\",\n       \"role\": \"SA\",\n       \"question\": \"...\",\n       \"answer\": \"...\"\n     }},\n     {{\n       \"title\": \"inferred_document_title\",\n       \"role\": \"DEV\",\n       \"question\": \"...\",\n       \"answer\": \"...\"\n     }},\n     {{\n       \"title\": \"inferred_document_title\",\n       \"role\": \"QA\",\n       \"question\": \"...\",\n       \"answer\": \"...\"\n     }}\n   ]\n   Text:\n   {text}\nExecution time: The complete generation took around couple of days.\n\n\n\nüõ†Ô∏è Domain adaption LoRA of base model\n\nModel Choice:\n\nFor speed and performance on my Mac M1, I used google/gemma-3-270m.\nDownloaded using Hugging Face transformers (both model & tokenizer).\n\n\nContext Window:\n\nUsed a context length of 512 tokens with an overlap of 128 tokens to preserve context across chunks.\nTotal tokens to be processed: 4,302,414.\n\nLoRA Configuration:\n\nTargeted only the attention block‚Äôs linear modules: q_proj, k_proj, v_proj.\nUsed rank = 16, lora_alpha = 32 for a good balance of capacity and speed.\nbias was not selected and drop out is set to 0.1\n\nTrain/Validation Split:\n\nData was split 95:5 into train and validation sets.\n\nBaseline Perplexity:\n\nIt is measured how well a language model predicts the next token in a seqence\nCalculated by finding conditional probabilty of next token wrt past tokens\nLower perplexity indicates the model has learned the input distribution better ‚Äî it is ‚Äúless surprised‚Äù by the text.\nOn-domain (wiki text): 35.87\nOut-of-domain (Wikipedia text): 52.28\nOut-of-domain evaluation was used to check for catastrophic forgetting during fine-tuning.\n\nTraining hyperparams:\n\nas all the inputs are of shpae 512\nbelow are the params py      batch_size = 4      gradient_accumulation_steps = 2 # keeping it low for not overflowing       learning_rate=1e-4              # an baseline learning_rate suggested by lora paper      num_train_epochs=3\nother hyper params are as follows:\n\n\nLoss landsscape:\n\nused trackio for tracking as keeping in the spirit of running in local\nThe loss graph is gradual as below.\n\n  &lt;div style=\"flex: 1; text-align: center;\"&gt;\n      &lt;img src=\"./static/blog3/base_train_loss.png\" alt=\"Training Loss\" width=\"600\"&gt;\n      &lt;p&gt;Training Loss&lt;/p&gt;\n  &lt;/div&gt;\n  &lt;div style=\"flex: 1; text-align: center;\"&gt;\n      &lt;img src=\"./static/blog3/base_eval_loss.png\" alt=\"Evaluation Loss\" width=\"600\"&gt;\n      &lt;p&gt;Evaluation Loss&lt;/p&gt;\n  &lt;/div&gt;\n\n\nResults After Fine-Tuning:\n\nOn-domain perplexity improved to 4.19 üéâ\nOut-of-domain perplexity improved slightly to 40.64 (no significant forgetting).\n\nSaving model:\n\nCombine the LoRA adapters with the base model to produce a single, unified model.\nSave the model and tokenizer for further processing\n\nVibe check:\n\nthe trained model is learning from the business wiki\nbelow is the inference\n\n\n\n\n\n‚ö° Further SFT on base model\n\nMessage Templete: For each question and answer pairs below is the input text. py     SYSTEM_PROMPT = 'You are a senior software developer. Answer truthfully and concisely.\\nIf unsure, reply \"I do not know.\" Explain steps briefly when needed.'     message = [         {\"role\": \"system\", \"content\": SYSTEM_PROMPT},         {\"role\": \"user\", \"content\": example['question']},         {\"role\": \"assistant\", \"content\": example['answer']}     ]\nContext Window:\n\nAim to find an appropriate context length is key. Too short the model misses to learn, too long adds unnecessary padding, wasting memory and computation.\nAs a rule of thumb selecting context window of 512 is idle for the formatted text. As it a whole no of power 2 to make GPU computational efficiently. \n\nLoRA Configuration:\n\nFor simplicity choosing same LoRA as base model.\n\nTrain/Validation Split:\n\nData was split 95:0.4:5 into train:valid:test sets. Smaller valid set for faster training.\nTest set is not used during training\n\nBaseline Entropy and Mean Token Accuracy:\n\nEntropy measures the model‚Äôs uncertainty ‚Äî lower entropy after SFT means the model is more confident about predicting the next token.\nMean Token Accuracy tracks how often the model predicts the correct next token, and should increase after SFT, showing better alignment with domain data.\nHF wiki refenece\n\n\nTraining hyperparams:\n\nbelow are the params py      batch_size = 8                              # for faster training and many input are below 100 tokens      gradient_accumulation_steps = 4                   learning_rate=1e-4                          # stable learning rate      max_length=512      num_train_epochs=1                          # increaing the epochs more than 1 leads to explosion of loss, and over flowing of loss      logging_steps, eval_steps, save_steps = 50, 50, 50\nother hyper params are as follows:\n\n\nLoss landsscape:\n\nThe loss graph is gradual as below.\n\n  &lt;div style=\"flex: 1; text-align: center;\"&gt;\n      &lt;img src=\"./static/blog3/SFT_train_loss.png\" alt=\"Training Loss\" width=\"600\"&gt;\n      &lt;p&gt;Training Loss&lt;/p&gt;\n  &lt;/div&gt;\n  &lt;div style=\"flex: 1; text-align: center;\"&gt;\n      &lt;img src=\"./static/blog3/SFT_eval_loss.png\" alt=\"Evaluation Loss\" width=\"600\"&gt;\n      &lt;p&gt;Evaluation Loss&lt;/p&gt;\n  &lt;/div&gt;\n\n\nResults After Fine-Tuning:\n\nAfter training the Entropy decresed to 2.44 and Mean Token Accuracy is improved to 57% üéâ\n\n\nSaving model:\n\nfollowed same approcah as base model\n\nVibe check:\n\nthe trained model is learning from the business wiki\nbelow is the a sample QA",
    "crumbs": [
      "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1"
    ]
  },
  {
    "objectID": "daftsft.html#conclusion",
    "href": "daftsft.html#conclusion",
    "title": "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1",
    "section": "Conclusion",
    "text": "Conclusion\nIn this experiment, I built a complete pipeline: scraping domain data from Confluence, generating synthetic Q&A data, and performing continual pretraining followed by SFT. The model showed decent results but still hallucinates and sometimes produces irrelevant text ‚Äî a clear signal that further refinement is needed. No efficent packing and speed up is not able to achived as training on Mac is painfully slow wrt Nvidia gpu.\nKey takeaways and next steps:\n\nImprove data diversity: As data is generated by a LLM, the performance will improve after adding more sythtic data and some real world opensource data for generalization.\nUse multiple LLMs for generation: Include multiple LLM for synthetic data generation which will give raises to more entropy of information\nExperiment with larger base models: Using bigger model for having grater knowledge absorption\nTune LoRA configs and hyperparameters: Having differenet configs for LoRA ranks and other hyper params in base model wihich will lead to storong adaption of source raw text.\nBuilding scheduler for SFT: The SFT triner fails dramtically when running with a decent lower lr i.e.¬†5e-5. To faster convergenece we have to use higher learning rate so that the model will not be stuck around local minima or saddle point.\nEval : a concrete pipeline for evalution of the trained model.",
    "crumbs": [
      "Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1"
    ]
  },
  {
    "objectID": "gptdecoder.html",
    "href": "gptdecoder.html",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "",
    "text": "üìÖ 15/10/2025",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#introduction-motivation",
    "href": "gptdecoder.html#introduction-motivation",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Introduction & Motivation",
    "text": "Introduction & Motivation\nAim: This blog documents the implementation of a Transformer architecture, focusing on the pretraining process that is critical and foundational to all LLMs. We use Shakespeare‚Äôs text as our dataset, keeping the setup lightweight enough to run on a single GPU in Google Colab‚Äôs free tier.\nTransformers: These are the neural network architecture powering modern language models. Originally introduced in the paper ‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017), transformers have become the foundation for models like GPT, BERT, and beyond.\nNote on Scope: This blog focuses exclusively on the pre-training phase of language models‚Äîteaching a transformer to predict the next character in a sequence. It does not covers the additional steps that make models like ChatGPT conversational and helpful, such as:\n\nSupervised Fine-Tuning (SFT) - teaching the model to follow instructions\nReinforcement Learning from Human Feedback (RLHF) - aligning the model with human preferences\n\nThink of this as ‚ÄúGPT‚Äù without the ‚ÄúChat‚Äù‚Äîwe‚Äôre building the foundational language understanding, which is the critical first step that all modern LLMs go through.",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#dataset-preprocessing",
    "href": "gptdecoder.html#dataset-preprocessing",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Dataset & Preprocessing",
    "text": "Dataset & Preprocessing\nA language model learns the joint probability distribution of words in a sentence‚Äîwhich is just a fancy way of saying it learns to predict the next word. For this step, big labs scrape massive amounts of text from the web and feed it to their models to learn from. There are big dataset like FineWeb edu or common crawl data. As I am extremely GPU poor, we will consider the Shakespeare dataset i.e.¬†some of the literature he produced. The data set is taken from the legendary Karpathy‚Äôs Nanogpt series. In fact this blog is a technical write up for the educational Zero to hero created by him.\nThe dataset consists of a giant text file, a big file dump. With all the text concatenated. Shakespeare dataset has about 1.1 million characters and 200k words‚Äîa nice manageable size for a toy transformer!\nThe dataset is devided by 90:10 ratio for training and valid respectively. Below is a example of dataset creation.\n\n\n\n\n\n\n\n\n\nFor the string ‚ÄúHello‚Äù, here‚Äôs how the autoregressive training creates examples:\nIndependent variable (Input/Context): The sequence of characters seen so far Dependent variable (Target/Output): The next character to predict\nLets consider examples from ‚ÄúHello‚Äù which generates 4 training sample :\n\nInput: ‚ÄúH‚Äù ‚Üí Target: ‚Äúe‚Äù\nInput: ‚ÄúHe‚Äù ‚Üí Target: ‚Äúl‚Äù\n\nInput: ‚ÄúHel‚Äù ‚Üí Target: ‚Äúl‚Äù\nInput: ‚ÄúHell‚Äù ‚Üí Target: ‚Äúo‚Äù\n\nEach example uses the previous characters (independent variable) to predict the next one (dependent variable).",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#tokenization",
    "href": "gptdecoder.html#tokenization",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Tokenization",
    "text": "Tokenization\nAny process involving languages goes through the Tokenization step. It‚Äôs basically the art of breaking text into smaller chunks that the model can work with. There are many ways to achieve this:\n\nCharacter-level: Split into individual characters\nWord-level: Split by words\nSubword-level: Split into meaningful chunks (BPE, WordPiece, etc.)\nByte-level: Split into bytes (used in GPT-2)\n\nThis implementation used Character-level. By using character-level tokenization, we keep things simple and lightweight. Since we‚Äôre modeling at the character level, the vocab (vocabulary‚Äîthe set of all possible tokens our model knows about) of our model is all the unique chars present in the text. There are around 65 unique chars in the dataset. Reasons for choosing this approach: simpler to implement, good for learning, and Shakespeare has a manageable vocab size.",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#embedding",
    "href": "gptdecoder.html#embedding",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Embedding",
    "text": "Embedding\n\nToken Embeddings\nEmbeddings are the individual vector representations we learn for each token in our vocabulary. This converts each token (character, in our case) into a multi-dimensional vector‚Äîtypically something like 64, 128, or 512 numbers‚Äîthat captures relationships between tokens. The model learns these vectors during training. Larger embedding dimensions lead to stronger learning of these patterns. For example, ‚Äòa‚Äô and ‚Äòe‚Äô might be used in similar contexts (both are vowels), so their embeddings become similar. These embeddings start as random numbers, and through training, the model gradually adjusts them to find optimal values that capture meaningful patterns. For the given itration of the expreiment, embedding dimensions is set to 128.\n\n\n\nPositional Embeddings\nOnce a sentence is converted to a list of tokens, these tokens are position invariant i.e.¬†are not aware what comes after what. positional embedding helps us in learning that. The positional embedding matrix has dimensions seq_len √ó embedding_dim, where each position gets its own vector that‚Äôs added to the token embedding. The sequence length (also called context length) is the window of text the model can ‚Äòremember‚Äô at once when predicting the next character. There are two main approaches: - Learned positional embeddings: Start random and are trained alongside the model (used in GPT) - Fixed sinusoidal embeddings: Use sine/cosine functions, not trainable (original Transformer paper)\nWe use learned positional embeddings‚Äîthese start as random values and get optimized during training, just like the token embeddings. The context length is set to 128. The input to the model input = token_embedding + positional_embedding. We add them (rather than concatenate) so the input stays compact and the model learns to blend what and where information together.\n\n\n\n\nInput to attention block.",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#causal-self-attention",
    "href": "gptdecoder.html#causal-self-attention",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Causal Self Attention",
    "text": "Causal Self Attention\nAttention: After embedding the inputs, the model needs to learn which previous tokens are most relevant for predicting the next one. For example, in ‚ÄôHello, my name is S____‚Äô, the model should pay attention to the broader context, not just the immediate previous character. The mechanism for building attention is to project the input into three different representations: Key (K), Query (Q), and Value (V). These are learned linear projections (just matrix multiplications that the model learns) of the input‚Äîin our case. While our embeddings are 128 dimensions, we project Q, K, V down to 16 dimensions each for this attention head. Below is a mental model for the idea behind them with respect to databases:\n\nKey (K): Index or primary key of a table\nQuery (Q): User asking questions\nValue (V): Answer to the user‚Äôs query\n\n\n\n\n\n\n\n\n\n\nJust like a database query finds relevant records, the Query helps find which tokens in our sequence are most relevant to the current position. The model compares each Query against all Keys to compute attention scores, which determine how much each token should attend to every other token. These scores are then used to create a weighted sum of the Values.\n\n\n\n\n\n\n\n\n\nclass AttentionHead(nn.Module):\n    def __init__(self, config:GPTConfig):\n        super().__init__()\n        assert config.embedding_dim % config.n_heads == 0\n        self.head_dim = config.embedding_dim // config.n_heads\n\n        self.Q_W = nn.Linear(config.embedding_dim, self.head_dim)       # weight of Q\n        self.K_W = nn.Linear(config.embedding_dim, self.head_dim)       # weight of K\n        self.V_W = nn.Linear(config.embedding_dim, self.head_dim)       # weight of V\n\n        mask = torch.tril(torch.ones(config.seq_len, config.seq_len))\n        self.register_buffer('mask', mask.masked_fill(mask == 0, float('-inf'))) # for building Causal mask\n\n        self.dropout = nn.Dropout(p = config.dropout)                    # randomly switching off some logits\n\n    def forward(self, x): #bs * seq_len * embedding_dim\n        Q, K, V = self.Q_W(x), self.K_W(x), self.V_W(x)                #bs * seq_len * head_dim\n        \n        attn = Q @ K.transpose(-2, -1) /  self.head_dim ** 0.5         #bs * seq_len * head_dim @ bs * head_dim * seq_len -&gt; bs * seq_len * seq_len\n        attn += self.mask[:x.shape[1], :x.shape[1]]\n\n        attn = torch.softmax(attn, dim=-1)\n        return self.dropout(attn @ V)                                  # bs * seq_len * seq_len @ bs * seq_len * head_dim -&gt; bs * seq_len *  head_dim",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#multi-head-attention",
    "href": "gptdecoder.html#multi-head-attention",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Multi-head attention",
    "text": "Multi-head attention\nA multi-head attention block runs multiple attention heads in parallel. Here‚Äôs what multi-head attention typically achieves: - Diversity: Each head can learn to attend to different patterns (one might focus on nearby characters, another on longer-range dependencies) - Richer representation: Combining multiple heads gives a more complete picture than a single attention mechanism\nThe outputs of all attention heads are concatenated together. Since each head outputs 16 dimensions and we have 8 heads, the concatenated result is back to our original 128 dimensions (8 √ó 16 = 128). This combined output is then fed through a final linear layer to mix the information from all the different attention perspectives. In the current setup, the number of heads is 8 (embedding_dim / head_dim = 128/16).\n\n\n\nMulti Attention Head.\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config:GPTConfig):\n        super().__init__()\n        assert config.embedding_dim % config.n_heads == 0 # config.n_heads * output of the embedding layer\n\n        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.n_heads)])\n        self.dropout = nn.Dropout(p=config.dropout)\n        self.linear = nn.Linear(config.embedding_dim, config.embedding_dim)\n        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n\n    def forward(self, x): #bs * seq_len * embedding_dim\n        head = torch.cat([head(x) for head in self.heads], dim=-1) #bs * seq_len * embedding_dim\n        head = self.dropout(self.linear(head))                     #bs * seq_len * embedding_dim\n        return self.layer_norm(head + x)                           #residual connections\nNote: 1. LayerNorm: A normalization technique that normalizes activations across the feature dimension, stabilizing training and helping gradients flow better. 1. Resblock: allow gradients to flow directly backward through the network, which is crucial for training deep models. Without them, gradients can vanish during backpropagation.",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#feed-forward-network",
    "href": "gptdecoder.html#feed-forward-network",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Feed-forward network",
    "text": "Feed-forward network\nThe output from multi-head attention is passed through a feed-forward network. After attention determines which tokens are relevant, the FFN processes and transforms that information at each position independently.\nThe FFN typically: - Expands the representation to a larger dimension (e.g., 128 ‚Üí 512), giving the model more capacity to learn complex patterns - Applies a non-linear activation function (like GELU or ReLU) to capture non-linear relationships. Which is a key aspect of any Nural network. - Projects back to the original dimension (512 ‚Üí 128). The output is of dim embedding_dim.\nThis expansion and contraction, combined with the non-linearity, allows the model to learn richer transformations of the attended features.\nclass FFN(nn.Module):\n    def __init__(self, config:GPTConfig):\n        super().__init__()\n\n        self.dropout = nn.Dropout(p=config.dropout)\n        self.linear1 = nn.Linear(config.embedding_dim, 4 * config.embedding_dim)\n        self.linear2 = nn.Linear(4 *config.embedding_dim, config.embedding_dim)\n        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n        self.gelu = nn.GELU(approximate='tanh')\n\n    def forward(self, x): #bs * seq_len * embedding_dim\n        pred = self.linear2(self.gelu(self.linear1(x)))\n        return self.layer_norm(self.dropout(pred) + x)\nThe transformer architecture is built by stacking multiple layers of these MHA ‚Üí FFN blocks. Each layer (or ‚Äútransformer block‚Äù) gets progressively better at understanding the input. Below is a rough struct.\n\nInput ‚Üí Embedding\nTransformer Block 1 (MHA ‚Üí FFN)\nTransformer Block 2 (MHA ‚Üí FFN)\nTransformer Block 3 (MHA ‚Üí FFN)\n‚Ä¶ (repeat N times)\nFinal output layer\n\nMore depth allows the model to learn more complex patterns in the text, but requires more compute resources and training time. In our setup, we use [N] transformer blocks.\nSingle block:",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#language-head",
    "href": "gptdecoder.html#language-head",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Language Head",
    "text": "Language Head\nIt takes in the output of the final attention meachanisim and return output of batch_size √ó seq_len √ó vocab_size. From where model can make predictions of next vocab using a single linear layer that projects from embedding_dim ‚Üí vocab_size. Each vocab has a logits or probability distribution.\nself.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#complete-model",
    "href": "gptdecoder.html#complete-model",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Complete model:",
    "text": "Complete model:",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#now-comes-the-fun-partteaching-our-model-to-actually-speak-shakespeare",
    "href": "gptdecoder.html#now-comes-the-fun-partteaching-our-model-to-actually-speak-shakespeare",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Now comes the fun part‚Äîteaching our model to actually speak Shakespeare!",
    "text": "Now comes the fun part‚Äîteaching our model to actually speak Shakespeare!\nTo train our model, we need a loss function a way to measure how wrong our predictions are. For language modeling Cross-entropy used which measures how far our predicted probability distribution is from the actual next character. If the actual next character is ‚Äòe‚Äô and our model gives it a 90% probability, that‚Äôs great! But if it only gives ‚Äòe‚Äô a 5% probability, the cross-entropy loss will be high, signaling the model needs to improve.\nHyperparameters\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nBatch Size\n256\nNumber of sequences processed in parallel\n\n\nSequence Length\n128\nContext window (max tokens the model can see)\n\n\nEmbedding Dimension\n128\nSize of token/positional embeddings\n\n\nNumber of Layers\n4\nTransformer blocks stacked\n\n\nNumber of Heads\n8\nAttention heads per block\n\n\nVocabulary Size\n65\nTotal unique characters in dataset\n\n\nDropout\n0.1\nDropout probability for regularization\n\n\nLearning Rate\n1e-3\nFixed learning rate for Adam optimizer\n\n\nMax Gradient Norm\n1.0\nGradient clipping threshold\n\n\nDevice\nCUDA/CPU\nAutomatic GPU detection\n\n\nDtype\nbfloat16/float16\nMixed precision training\n\n\nepochs\n75\nNo of steps the training is done\n\n\n\nTraining setup: I used the Adam optimizer with a fixed learning rate of 1e-3 and trained for 75 epochs. On Colab‚Äôs free tier GPU, this took a while (grab a coffee!), but it‚Äôs totally doable without fancy hardware.",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#generation-inference",
    "href": "gptdecoder.html#generation-inference",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Generation (Inference)",
    "text": "Generation (Inference)\nNow that our model is trained, let‚Äôs make it generate some Shakespeare! The function takes a prompt (like ‚ÄúTo be or not to be‚Äù) and predicts characters one at a time.\nKey parameters: - max_new_tokens: How many new characters to generate - temperature: Controls randomness. Higher values (like 1.5) make it more creative/chaotic, lower values (like 0.5) make it more predictable and coherent\nHow it works: 1. Start with your prompt, convert it to tokens 2. Feed it through the model to get predictions for the next character 3. Sample a character based on those predictions (with some randomness controlled by temperature) 4. Add that character to the sequence and repeat\nThe model keeps the conversation going character by character until it hits the max_new_tokens limit. It‚Äôs autoregressive‚Äîeach new prediction depends on everything that came before!\n@torch.no_grad() \ndef generate(prompt, max_new_tokens=100, temperature=1.0):\n    \"\"\"\n    prompt: string to start generation\n    max_new_tokens: how many tokens to generate\n    temperature: higher = more random, lower = more deterministic\n    \"\"\"\n    model.eval()\n    tokens = tokenizer.encode(prompt)\n    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dim\n    tokens = tokens.to('cuda')\n    for _ in range(max_new_tokens):\n        # Crop to last seq_len tokens if needed\n        context = tokens if tokens.size(1) &lt;= model.embed.pos_ids.size(0) else tokens[:, -model.embed.pos_ids.size(0):]\n\n        # Get predictions\n        with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n          logits = model(context)\n        logits = logits[:, -1, :] / temperature  # Focus on last token\n\n        # Sample next token\n        probs = torch.softmax(logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)\n\n        # Append to sequence\n        tokens = torch.cat([tokens, next_token], dim=1)\n\n    return tokenizer.decode(tokens.squeeze().tolist())\nprint(generate(\"To be or not to be\"))",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#conclusion",
    "href": "gptdecoder.html#conclusion",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Conclusion",
    "text": "Conclusion\nThis project successfully implemented a character-level transformer model trained on Shakespeare‚Äôs text. The model learned to generate coherent, Shakespeare-style text by predicting characters one at a time. The model demonstrates the power of the transformer architecture even in a resource-constrained setting (single GPU on Colab‚Äôs free tier).\nWhat worked well: - The attention mechanism effectively captured dependencies between characters - Character-level tokenization kept the vocabulary small (65 tokens) and manageable - The model converged and produced recognizable Shakespeare-style patterns\nAreas for improvement:\nSeveral optimization techniques were not implemented in this version but could significantly improve performance:\n\nProper weight initialization: The layers were not initialized optimally, which may have slowed convergence and required more training epochs.\nLearning rate scheduling: A fixed learning rate was used throughout training. Implementing a learning rate schedule (such as cosine decay or warmup followed by decay) would allow the model to take larger steps early in training and fine-tune more carefully later.\nRegularization: Weight decay in the Adam optimizer was not properly tuned, which could help prevent overfitting and improve generalization.\n\nNext steps:\nFuture iterations could explore more advanced techniques used in modern language models:\n\nRoPE (Rotary Position Encoding): A more effective positional encoding scheme used in models like LLaMA\nGrouped Query Attention (GQA): An efficient attention variant that reduces memory usage while maintaining performance\nFlash Attention: Optimized attention implementation for faster training\nMultimodal training: Extending beyond text to handle multiple data types\n\nThis project serves as a solid foundation for understanding how transformers work under the hood and provides a stepping stone toward implementing more sophisticated architectures.",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "gptdecoder.html#reference",
    "href": "gptdecoder.html#reference",
    "title": "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation",
    "section": "Reference",
    "text": "Reference\n\nAttention is All You Need - The original transformer paper Vaswani et al., 2017\nAndrej Karpathy‚Äôs ‚ÄúLet‚Äôs build GPT‚Äù - The video tutorial you followed\nShakespeare dataset\nGPT2 paper: Improving Language Understanding by Generative Pre-Training",
    "crumbs": [
      "Building a Text Only Nano-GPT from Scratch: Character-Level Shakespeare Generation"
    ]
  },
  {
    "objectID": "llmforwardpass.html",
    "href": "llmforwardpass.html",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "",
    "text": "üìÖ 31/08/2025\nEver wondered what happens when you type ‚ÄúThe cat is‚Äù and an AI completes it with something like ‚Äúsitting on the windowsill‚Äù? You‚Äôre about to peek behind the curtain and see exactly how Large Language Models think, predict, and generate text - one token at a time.",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#what-well-discover",
    "href": "llmforwardpass.html#what-well-discover",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "What We‚Äôll Discover",
    "text": "What We‚Äôll Discover\nIn this hands-on journey, we‚Äôll build our own mini text generator and watch it work in real-time. You‚Äôll see: - How text becomes numbers (and back again) - Why the model considers 49,152 possibilities for every single word - How controlled randomness prevents boring, repetitive responses - The simple loop that powers every AI conversation\nReady to demystify the magic? Let‚Äôs dive in!",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#setup",
    "href": "llmforwardpass.html#setup",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "setup",
    "text": "setup\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = 'HuggingFaceTB/SmolLM2-135M-Instruct'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#tokenization",
    "href": "llmforwardpass.html#tokenization",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "Tokenization",
    "text": "Tokenization\nTokenization is a fundamental step in text processing and natural language processing (NLP). A tokenizer breaks down text into smaller, meaningful units called ‚Äútokens.‚Äù Think of it like taking a sentence and splitting it into individual words, or even smaller pieces depending on your needs. For example, the sentence ‚ÄúHello, world!‚Äù might be tokenized into:\n\n[‚ÄúHello‚Äù, ‚Äú,‚Äù, ‚Äúworld‚Äù, ‚Äú!‚Äù] (word-level tokens)\nOr even [‚ÄúHel‚Äù, ‚Äúlo‚Äù, ‚Äú,‚Äù, ‚Äúwor‚Äù, ‚Äúld‚Äù, ‚Äú!‚Äù] (subword tokens)\n\n\ntext = \"The cat is \"\ntokens = tokenizer(text)\ntokens\n\n{'input_ids': [504, 2644, 314, 216], 'attention_mask': [1, 1, 1, 1]}\n\n\n\ninput_ids: These are the numerical IDs that represent each token - this is what the model actually processes\nattention_mask: Other inputs required for the model, let‚Äôs ignore it for now\n\n\n# to get back token str from the tokens\n[tokenizer.decode([token_id]) for token_id in tokens['input_ids']]\n\n['The', ' cat', ' is', ' ']\n\n\nToken breakdown: 1. 'The', ' cat', ' sat', ' ': other words and their corresponding representations. We can see that the tokenizer doesn‚Äôt just split on spaces. It learns patterns from training data, so spaces become part of tokens (except the first word).\nWhy Subword Tokenization? You might wonder why ‚Äúcat‚Äù becomes ‚Äù cat‚Äù (with a space). Modern tokenizers use ‚Äúsubword‚Äù tokenization - they learn common patterns from millions of texts. A space before a word often signals it‚Äôs a separate concept, so the tokenizer treats ‚Äù cat‚Äù as one unit. This helps the model understand word boundaries and context better than just splitting on spaces.\n\nprint(tokenizer.special_tokens_map)\nprint(tokenizer.vocab_size)\n\n{'bos_token': '&lt;|im_start|&gt;', 'eos_token': '&lt;|im_end|&gt;', 'unk_token': '&lt;|endoftext|&gt;', 'pad_token': '&lt;|im_end|&gt;', 'additional_special_tokens': ['&lt;|im_start|&gt;', '&lt;|im_end|&gt;']}\n49152\n\n\nThere are 49152 unique tokens are there in the tokenizer.",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#the-foundation-from-words-to-numbers",
    "href": "llmforwardpass.html#the-foundation-from-words-to-numbers",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "The Foundation: From Words to Numbers",
    "text": "The Foundation: From Words to Numbers\nThe Model‚Äôs Internal Thinking Process When you see those logits numbers, you‚Äôre literally looking at the model‚Äôs ‚Äúthoughts‚Äù! Each position in our input gets its own set of predictions. The model isn‚Äôt just guessing the next word - it‚Äôs considering what could come after EVERY position. But for text generation, we only care about the very last position (after ‚Äù is‚Äù).\nWe have converted the text to tokens and will use them to get the next token from the model.\n\nimport torch\ninput_tensor = torch.tensor([tokens['input_ids']]) # as the model needs ptorch tensor as input\nop = model(input_tensor)\n\nWe have to focus on logits for now and can ignore all other parts.\n\nop.logits\n\ntensor([[[16.3669,  8.8479, 12.6537,  ..., 10.0735, 13.1624,  5.4319],\n         [19.1141, 12.7332, 15.7778,  ..., 20.1228, 19.1875, 10.3858],\n         [10.1939,  1.3824,  4.2152,  ..., 13.2681, 12.5120,  2.1723],\n         [16.5672,  7.1412, 11.3807,  ..., 14.1004, 13.9356,  7.9265]]],\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\n\n\n\nop.logits.shape\n\ntorch.Size([1, 4, 49152])\n\n\nWhy So Many Numbers? 49,152 might seem like overkill, but remember - the model has to consider EVERY possible token it knows. This includes common words like ‚Äúhappy‚Äù, rare words like ‚Äúsesquipedalian‚Äù, numbers, punctuation, and even tokens from other languages. Most will have very low scores, but the model still evaluates them all.\n\nprint(f\"Logits shape: {op.logits.shape}\")\nprint(\"Shape breakdown: [Batch_size, Sequence_length, Vocab_size]\")\nprint(f\"[{op.logits.shape[0]}, {op.logits.shape[1]}, {op.logits.shape[2]}]\")\nprint(f\"- Batch: {op.logits.shape[0]} text(s) processed\")\nprint(f\"- Sequence: {op.logits.shape[1]} tokens in input\") \nprint(f\"- Vocab: {op.logits.shape[2]:,} possible next tokens\")\n\nLogits shape: torch.Size([1, 4, 49152])\nShape breakdown: [Batch_size, Sequence_length, Vocab_size]\n[1, 4, 49152]\n- Batch: 1 text(s) processed\n- Sequence: 4 tokens in input\n- Vocab: 49,152 possible next tokens",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#possibilities-watching-the-model-think-from-raw-scores-to-probabilities",
    "href": "llmforwardpass.html#possibilities-watching-the-model-think-from-raw-scores-to-probabilities",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "49,152 Possibilities: Watching the Model Think From Raw Scores to Probabilities",
    "text": "49,152 Possibilities: Watching the Model Think From Raw Scores to Probabilities\n\nConverting logits to probabilities using softmax\nDemonstrating why we can‚Äôt just pick the highest logit every time\nA simple example of sampling vs greedy selection\n\n\n# Extract logits for the last token position (where next token will be predicted)\nlast_token_logits = op.logits[:, -1, :]  # Shape: [1, 262144]\n\n# Find the token with highest probability (greedy selection)\npredicted_token_id = last_token_logits.argmax(dim=-1)  # Gets index of max value\n\n# convert the id to token\nnext_token = tokenizer.decode(predicted_token_id)\n\nprint(f\"predicted_token_id : {predicted_token_id.item()}\")\nprint(f\"next token : `{next_token}`\")\n\npredicted_token_id : 33\nnext token : `1`",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#the-art-of-selection-why-randomness-matters",
    "href": "llmforwardpass.html#the-art-of-selection-why-randomness-matters",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "The Art of Selection: Why Randomness Matters",
    "text": "The Art of Selection: Why Randomness Matters\nSo far, we‚Äôve done text to predictions. But here‚Äôs the thing - if we always select the token with the highest logit score (greedy selection), our model becomes predictable and boring. It‚Äôs like having a conversation with someone who always gives the most obvious response!\nThis is where sampling comes in. Instead of always picking the #1 choice, large language models introduce some randomness by selecting from the top-K highest scoring tokens, where K is a number we can control.\nThere are two parameters are used, below are 1. Temperature is like a creativity dial: 0.1 = very predictable, 1.5 = very creative 1. Top-k means ‚Äòonly consider the k most likely tokens‚Äô - saves computation and improves quality\n\nimport torch.nn.functional as F\n\ndef generate_next_token(text, temperature=1.0, top_k=50):\n    \"\"\"Simple function to show one step of text generation\"\"\"\n    # Tokenize input\n    tokens = tokenizer(text, return_tensors=\"pt\")\n    \n    # Get model predictions\n    with torch.no_grad():\n        outputs = model(**tokens)\n    \n    # Get logits for next token prediction\n    next_token_logits = outputs.logits[0, -1, :] / temperature\n    \n    # Get top-k most likely tokens\n    top_logits, top_indices = torch.topk(next_token_logits, top_k)\n    \n    # Convert to probabilities and sample \n    probs = F.softmax(top_logits, dim=-1)\n\n    # randomly sample from all the probabilities\n    next_token_idx = torch.multinomial(probs, 1)\n    \n    next_token_id = top_indices[next_token_idx]\n    \n    return tokenizer.decode(next_token_id)\n\n\n[generate_next_token(\"The cat is\", 0.7) for _ in range(4)]\n\n[' very', ' a', ' standing', ' also']\n\n\nNotice how we get different tokens each time? That‚Äôs the beauty of sampling - it prevents boring, repetitive text! Think of temp as a awesome lever to select random tokens where is top_p is for setting up the window to consider.",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#putting-it-all-together-building-our-generator",
    "href": "llmforwardpass.html#putting-it-all-together-building-our-generator",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "Putting It All Together: Building Our Generator",
    "text": "Putting It All Together: Building Our Generator\nWe have to add in new tokens to the end of the text and pass it to the model.\n\ndef generate_text(prompt, max_tokens=10):\n    current_text = prompt\n    for i in range(max_tokens):\n        next_token = generate_next_token(current_text, temperature=0.7)\n        current_text += next_token\n        print(f\"Step {i+1}: {current_text}\")\n    return current_text\n\n\ngenerate_text(text, 4)\n\nStep 1: The cat is 1\nStep 2: The cat is 10\nStep 3: The cat is 100\nStep 4: The cat is 100%\n\n\n'The cat is 100%'",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "llmforwardpass.html#the-magic-revealed-what-weve-learned",
    "href": "llmforwardpass.html#the-magic-revealed-what-weve-learned",
    "title": "The Magic Behind the Curtain: How LLMs Actually Generate Text",
    "section": "The Magic Revealed: What We‚Äôve Learned",
    "text": "The Magic Revealed: What We‚Äôve Learned\nCongratulations! You‚Äôve just built and understood the core engine that powers every large language model. What seemed like magic is actually a surprisingly elegant process:\nThe Complete Picture:\n\nText becomes numbers - Tokenization converts human language into mathematical representations\nPattern recognition at scale - The model evaluates 49,152 possibilities for every single prediction\nControlled randomness - Temperature and top-k sampling prevent boring, repetitive outputs\nIterative generation - This simple loop repeats to create coherent, contextual text\n\nWhy This Matters: Every time you chat with ChatGPT, Claude, or any AI assistant, this exact process runs behind the scenes. The model isn‚Äôt ‚Äúthinking‚Äù in human terms - it‚Äôs performing incredibly sophisticated pattern matching based on billions of text examples it learned from.\nThe Bigger Picture: This same fundamental process scales from our tiny 135M parameter model to massive systems with hundreds of billions of parameters. The core loop remains the same: predict, sample, add, repeat.\nUnderstanding this gives you insight into why LLMs sometimes hallucinate (they‚Äôre optimizing for plausible patterns, not truth), why they can be creative (controlled randomness), and why context matters so much (each prediction builds on everything before it).",
    "crumbs": [
      "The Magic Behind the Curtain: How LLMs Actually Generate Text"
    ]
  },
  {
    "objectID": "multimodal.html",
    "href": "multimodal.html",
    "title": "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST",
    "section": "",
    "text": "üìÖ 15/10/2025\nMy Nano GPT model could write Shakespeare-style text. Cool. But it had no idea what a boot looked like. Time to teach it.\nBut here‚Äôs the problem: images and text are fundamentally different beasts. Text? Nice, tidy tokens. Images? Thousands of pixels, each with values from 0-255. The model needs to make sense of both simultaneously.\nAim: Following the spirit of faster iteration and rapid experimentation, I built a text-to-text model (takes text input, predicts text output). The goal: expand this to handle multimodal inputs‚Äîlearning to generate text captions from images.\nThe experiment uses Shakespeare text for language modeling and Fashion-MNIST images with captions for vision-language tasks. This architecture pattern is similar to modern models like LLaVA. The model learns that üë¢ ‚Üí 'simple boot'\nBy the end of this blog, you‚Äôll see how this approach can be extended to different modalities‚Äîaudio, medical images, and more.",
    "crumbs": [
      "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST"
    ]
  },
  {
    "objectID": "multimodal.html#data-preparation",
    "href": "multimodal.html#data-preparation",
    "title": "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST",
    "section": "Data Preparation",
    "text": "Data Preparation\nThere are two types of data that we have to prepare 1. Text to Text 1. Vison to Text\nBefore jumping to each type of data. Lets discuss the tokenizer. I used char level tokenizer(detailed discussed in the Nano-GPT tokenizer. There are 65 tokens. As we are expanding the learning to new modality there are two new tokens are used\n\npadding: fills shot tokens to make uniform length\nend of sentence: to indicates model to stop generation\n\nFor the vision-to-text task, apart for its original role \\n serves two additional roles: marking the end of a caption (so the model knows when to stop generating) and padding shorter captions to uniform length.\n\nText to Text\nThe data for the training is Shakespeare text. The dependent and independent variables are a token list and next tokens. The detailed discussion is done by my Nano-GPT blog.\n\n\nVision to Text\nFor vision, keeping with our ideal of faster iteration, I used the Fashion MNIST dataset, which is for fashion image classification. The dataset consists of grayscale images of size 28√ó28, and there are 10 types of images. The inputs to the model in the scenario are following:\nThe input format for vision tasks: - Input: Image (1√ó28√ó28 tensor) + text caption tokens\n- Target: Next tokens, shifted caption with newline \\n as End of Setence tokens(which model will learn caption and stop generation)\nFor example, an image of a boot paired with caption tokens for ‚Äúsimple boot‚Äù will predict ‚Äúsimple boot‚Äù. This way, the model learns to generate captions autoregressively, just like it generates Shakespeare text.\n\n\n\n\n\n\n\n\n\n\n\nDataset Statistics\nThe model trains on two distinct data sources with different characteristics:\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nType\nTraining Samples\nValidation Samples\nBatch Size\nTraining Batches\nValidation Batches\n\n\n\n\nShakespeare\nText-to-Text\n~1M chars\n~100K chars\n64\n123\n14\n\n\nFashion-MNIST\nVision-to-Text\n60,000 images\n10,000 images\n512\n118\n20\n\n\n\nThe similar number of training batches (123 vs 118). This balancing ensures the model gets roughly equal training opportunities on both modalities‚Äîmore details on the alternating iterator in the Training section.\nA sample for image, input and target caption",
    "crumbs": [
      "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST"
    ]
  },
  {
    "objectID": "multimodal.html#arch",
    "href": "multimodal.html#arch",
    "title": "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST",
    "section": "Arch",
    "text": "Arch\nThe model have three primary units out of which two system for processing image.\n\nGPT2: The original arch for decoder block, for text-to-text model\nvision encoder: to encode image to lower dimensions\nprojection layer: convert encoded image to required dimension for decoder embeddings\n\n\n\n\nFinal Model\n\n\n\nGPT2\nTransformer decoder model for understanding and generating text. Which takes in tokens embedding with the positional embedding to process further by the model. Both the embedding is of shape 128. The complete example for the same is disscussed in details Nano-gpt model. The arch as well as the hyper params are kept same which is used for shakespere model.\nHyperparameters\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nBatch Size\n256\nNumber of sequences processed in parallel\n\n\nSequence Length\n128\nContext window (max tokens the model can see)\n\n\nEmbedding Dimension\n128\nSize of token/positional embeddings\n\n\nNumber of Layers\n4\nTransformer blocks stacked\n\n\nNumber of Heads\n8\nAttention heads per block\n\n\nVocabulary Size\n65\nTotal unique characters in dataset\n\n\nDropout\n0.1\nDropout probability for regularization\n\n\nLearning Rate\n1e-3\nFixed learning rate for Adam optimizer\n\n\nMax Gradient Norm\n1.0\nGradient clipping threshold\n\n\nDevice\nCUDA/CPU\nAutomatic GPU detection\n\n\nDtype\nbfloat16/float16\nMixed precision training\n\n\n\nFor text all the positional encoding are used. In text-only mode, tokens use positions [0:n]. In multimodal mode, the image embedding takes position [0], and text tokens shift to positions [1:n]. This ensures the image context is visible to all subsequent tokens through casual attention.\n\n\nVision Encoder: Teaching the Model to ‚ÄúSee‚Äù\nThe GPT decoder speaks the language of embeddings‚Äîvectors of size 128. But images? They‚Äôre 28√ó28 pixels. We need a translator. It‚Äôs like the encoder speaks ‚Äúpixel‚Äù and the decoder speaks ‚Äúmeaning‚Äù‚Äîwe need a bilingual friend.\nEnter the Vision Encoder: A ResNet-style CNN that compresses Fashion-MNIST images (1√ó28√ó28) down to a 512-dimensional feature vector. Think of it as converting raw pixels into a ‚Äúsemantic summary‚Äù the decoder can understand.\nArchitecture:\nImage (1√ó28√ó28)\n  ‚Üì ResBlock: 1‚Üí64 channels, 28‚Üí14 spatial\n  ‚Üì ResBlock: 64‚Üí128 channels, 14‚Üí7 spatial  \n  ‚Üì ResBlock: 128‚Üí256 channels, 7‚Üí4 spatial\n  ‚Üì ResBlock: 256‚Üí512 channels, 4‚Üí2 spatial\n  ‚Üì AdaptiveAvgPool ‚Üí (512,)\n  ‚Üì Flatten ‚Üí (512)\nThe 512-dimensional output, it‚Äôs a sweet spot(not too large nor too small). Modern vision encoders (like CLIP) use 512-768 dimensions for similar reasons. It‚Äôs enough to capture ‚Äúboot with laces‚Äù vs ‚Äúankle boot‚Äù without encoding every pixel‚Äôs cousin.\nWhy ResBlocks? They use skip connections‚Äîadding the input directly to the output. This helps gradients flow during training and lets the network learn both ‚Äúwhat changed‚Äù and ‚Äúwhat stayed the same.‚Äù Modern arch uses VIT(Vision Transformer) for encoding with same core principle. It have larger compression and efficency of learning. But CNN are smaller and faster to itreation. The encoder learns edges and other other shape present in the image. The final layer known as classification head which takes in the embedding and predicts the object. It is dicarded after pretraining only encoder is kept.\nTraining: First, I trained this encoder as a standalone classifier (with a 512‚Üí1024‚Üí10 classification head) on Fashion-MNIST. After 15 epochs: 98.9% accuracy.\nThen I froze it. Think of it like hiring a pre-trained photographer. They already know how to ‚Äúsee‚Äù fashion items‚Äîwe‚Äôre just teaching the writer (decoder) how to describe their photos. The encoder‚Äôs weights stay fixed, which decreases computational cost (no gradients for the encoder block). We just need to project those 512-dim features to the decoder‚Äôs input. If we update the weight of the vision encoder during final training during the captioning, might leads to catastrophic forgetting.\n\n\n\nThe Vison Encoder\n\n\nWhere each ResBlock invoked from\nclass ResBlock(nn.Module):\n    def __init__(self, ni, nf, ks=3, stride=2):\n        \"\"\"\n        Args:\n            ni: number of input channels\n            nf: number of output channels (filters)\n            ks: kernel size (default 3)\n            stride: stride for first conv (default 2 for downsampling)\n        \"\"\"\n        super().__init__()\n        # First conv: changes channels and spatial dims\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride),\n            nn.BatchNorm2d(nf))\n\n        # Second conv: keeps channels and spatial dims constant\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(nf, nf, ks, padding=ks//2, stride=1),\n            nn.BatchNorm2d(nf))\n\n        # Handle dimension mismatch\n        self.skip = nn.Conv2d(ni, nf, 1, stride=stride) if ni != nf else nn.Identity()\n\n    def forward(self, x):\n        # Add skip connection to output of two convs\n        return F.relu(self.skip(x) + self.conv2(F.relu(self.conv1(x))))\n\n\nProjection layer\nIt acts as a bridge for vision encoder and the GPT2‚Äôs embedding layer. It projects the input of shape 512 to embedding of dim 128, uses a simple linear layer.\n\n\nFinal Model\nThe final model as follows. The forward pass logic is slightly changed to accomodate the image. - When image is present: image ‚Üí encoder ‚Üí projection ‚Üí add pos[0] ‚Üí concat with text embeddings. 1. Token at pos[1] can see: [image] 1. Token at pos[2] can see: [image, token‚ÇÅ] 1. Token at pos[3] can see: [image, token‚ÇÅ, token‚ÇÇ] 1. And so on‚Ä¶\nThe generated tokens should have the reference to the image. \n\nWhen Text is only present: just text embeddings. Complete tokens are passed.\n\nclass MultiModal(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.vis_encoder = classifier[0]\n        self.proj_layer = nn.Linear(visConfig.head_op_dim, gptConfig.embedding_dim)\n\n        self.embed = Embedding(gptConfig)\n        self.blocks = nn.ModuleList(\n            [\n                nn.Sequential(MultiHeadAttention(gptConfig), FFN(gptConfig))\n                for _ in range(gptConfig.n_layers)\n            ])\n        self.layer_norm = nn.LayerNorm(gptConfig.embedding_dim)\n        self.lm_head = nn.Linear(gptConfig.embedding_dim, gptConfig.vocab_size)\n\n        for param in self.vis_encoder.parameters():\n            param.requires_grad = False\n\n    def forward(self, text_idx, image=None):\n        if image is not None:\n            # Ensure image has the correct dtype before passing to the encoder\n            image = image.to(self.proj_layer.weight.dtype)                     # ensure the image input has the correct data type\n            img_emb = self.proj_layer(self.vis_encoder(image)).unsqueeze(1)    # (bs, 1, 128)\n            img_emb = img_emb + self.embed.pos_embed(self.embed.pos_ids[0:1])  # fetch embeddings at the 0th idx\n            text_emb = self.embed(text_idx, start_idx=1)                       # positions start at 1\n            x = torch.cat([img_emb, text_emb], dim=1)\n        else:\n            x = self.embed(text_idx)\n\n        for block in self.blocks:\n            x = block(x)\n        return self.lm_head(self.layer_norm(x))",
    "crumbs": [
      "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST"
    ]
  },
  {
    "objectID": "multimodal.html#training",
    "href": "multimodal.html#training",
    "title": "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST",
    "section": "Training",
    "text": "Training\nThe loss function is used same as Nano-GPT i.e.¬†CrossEntropyLoss. The combined training loss (averaged across both tasks) decreases from 1.66 to 0.78 after 30 epochs. However, tracking each task separately reveals very different learning dynamics. As the number of batches for vison and text are close to each other this gives the model to equal chance to learn. The training uses a custom TrainBatchIter that alternates between Shakespeare text batches and Fashion-MNIST vision batches (text‚Üívision‚Üítext‚Üívision‚Ä¶). This ensures the model gets equal exposure to both modalities throughout training. Which gives the model equal chance to learn. The dataloders can be mixed but it would further complecate the forward pass of the MultiModal.\nThe model shows interesting behavior across the two tasks. Starting from similar initial losses (Text: 4.43 | Vision: 4.48), the vision-to-text task converges dramatically faster. After 30 epochs, vision loss drops to 0.17 (97% reduction) while text loss reaches 1.44 (68% reduction). This makes sense: captioning 10 Fashion-MNIST classes with short phrases is simpler than mastering Shakespeare‚Äôs vocabulary and style. The pretrained vision encoder (already 98.9% accurate) does most of the heavy lifting‚Äîthe model just learns to translate those visual features into words.\nOther Hyperparameters remains same Nano-GPT.",
    "crumbs": [
      "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST"
    ]
  },
  {
    "objectID": "multimodal.html#inference",
    "href": "multimodal.html#inference",
    "title": "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST",
    "section": "Inference",
    "text": "Inference\n\nImage Inference\nThe function takes in image and stops generation till the \\n is generated.\n@torch.no_grad()\ndef generate_caption(image, max_len=30):\n    model.eval()\n    image = image.unsqueeze(0).to(multiConfig.device).to(multiConfig.dtype)  # Add batch dimension and move to device with correct dtype\n\n    generated = []\n    text_idx = torch.empty((1, 0), dtype=torch.long, device=multiConfig.device)  # Empty text\n\n    for _ in range(max_len):\n        logits = model(text_idx, image)\n        next_token = logits[:, -1, :].argmax(dim=-1)\n\n        # Check for stop token '\\n'\n        if tokenizer.decode([next_token.item()]) == '\\n':\n            break\n\n        generated.append(next_token.item())\n        text_idx = torch.cat([text_idx, next_token.unsqueeze(0)], dim=1)\n\n    return tokenizer.decode(generated)\n\n\n\nInferenece\n\n\nThe model predicts caption casual sweater where as the original caption is knit sweater. It all belongs to same class so the classification is working properly.\n\n\nText Generation\nThe model able to learn Shakespeare text. Below is the text generated. The logic remains same as generation function in Nano GPT.\nTo be or not to beast.\nAnd gave me resolved them and lips to hear.\nThis hath prevent so you and thou, I were, good their\nFrozing in a curse; and acchive of a blous,\nWhose as hear me, thank, over with wind fair,\nAnd against the pave of him.\n'Duke his wrongly souls, holy, and",
    "crumbs": [
      "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST"
    ]
  },
  {
    "objectID": "multimodal.html#conclusion",
    "href": "multimodal.html#conclusion",
    "title": "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST",
    "section": "Conclusion",
    "text": "Conclusion\nA quick recap: I built a multimodal model that absorbs both text and image caption data. Although the vision encoder uses CNNs‚Äîa relatively old architecture compared to the Vision Transformers (ViT) used in recent LLMs‚Äîthe core principles remain the same.\nThe ‚ÄúAha‚Äù Moments:\nThe biggest surprise? How fast learning happens with the right setup. The vision task converged in just a few epochs (loss: 4.48 ‚Üí 0.17), while text took longer (4.43 ‚Üí 1.44). Three factors made this possible:\n\nEffective batching - Alternating text/vision batches gave equal learning opportunities\nSmart preprocessing - Pre-training the vision encoder separately, then freezing it\nSynthetic data - Fashion-MNIST with generated captions is small enough to iterate quickly, yet rich enough to learn multimodal alignment\n\nFrom Papers to Practice:\nMultimodal papers can feel daunting‚Äîbillions of parameters, massive datasets, distributed training. But here‚Äôs the truth: the principles scale down beautifully. By starting with Fashion-MNIST (60K images) and Shakespeare the model convergences faster.\nThis is the power of first principles. LLaVA uses CLIP + LLaMA. I used ResNet + GPT. Different scale, same idea: freeze a vision encoder, project to language space, let the decoder learn to describe what it sees.\nWhat‚Äôs Next?\n\nUpgrade the vision encoder - Replace ResNet with ViT (once I learn it!)\nMore modalities - Audio, medical images, time-series data\nLarger datasets - COCO captions\n\nThe architecture is ready. The principles are proven. Now it‚Äôs just a matter of scale.",
    "crumbs": [
      "Building a Multi Modal Nano-GPT from Scratch: Shakespeare Meets Fashion MNIST"
    ]
  }
]