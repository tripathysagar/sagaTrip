{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8efe57cb",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: agentfromfirstprinciple.html\n",
    "title: Agent From First Principle\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119473d0",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "  <span>ğŸ“… 15/11/2025</span>\n",
    "    <p align=\"right\">\n",
    "    <a href=\"https://colab.research.google.com/github/tripathysagar/sagaTrip/blob/main/nbs/08_AgentFromFirstPrinciple.ipynb\" target=\"_blank\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee375443",
   "metadata": {},
   "source": [
    "\n",
    "In the age of LLM, there is a big explosion of autonomy populary known as Agentic systems. An agent, in the LLM context, is indeed about giving the model capabilities beyond just generating text. While an LLM can produce intelligent responses, an agent framework allows it to take actions, use tools, and interact with external systems.\n",
    "\n",
    "Think of it this way:\n",
    "- **LLM alone**: Can reason and generate text, but is isolated\n",
    "- **LLM as an agent**: Can use tools, call APIs, execute code, access databases, and take actions based on its reasoning\n",
    "\n",
    "Aim: There are many lib which tailors to various niche areas for this activity. Some are boated comes with steep learning curve. Here we are going to implement agentic system for **file search agent** an LLM from ground up. Our goal is to search your filesystem using tools like `find`, `grep`, and `ls`.\n",
    "```text\n",
    "We will learn how agents work by implementing one ourself, understanding:\n",
    "- How to give LLMs access to tools\n",
    "- How to handle tool calls safely\n",
    "- How to create an agent loop that reasons and acts\n",
    "```\n",
    "\n",
    "We have to implement everything from scratch from first principle. For llm calling we will use `litellm` a thing wrapper and aggregator, `toolslm` for tool calling helper. The model to use `groq/openai/gpt-oss-20b`.\n",
    "\n",
    "In this experiment I am using Groq for LLM inferenece, any other LLM system with tool calling can be used in its place. I have already setup the api from [Groq](https://console.groq.com/keys) and set it as env variable name `GROQ_API_KEY`. If you are useing colab add this in secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e1112",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a9e1112",
    "outputId": "3a763055-1b29-443f-a592-df80b46abad0",
    "time_run": "8:12:31p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/10.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/10.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/10.4 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/278.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/64.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "# install dependecies\n",
    "!pip install -q litellm toolslm lisette fastcore graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac5d4c",
   "metadata": {
    "id": "b6ac5d4c",
    "time_run": "8:04:42p"
   },
   "outputs": [],
   "source": [
    "from lisette.core import mk_msg\n",
    "import litellm\n",
    "import os\n",
    "\n",
    "model_name = \"groq/openai/gpt-oss-20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346dd5c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "346dd5c2",
    "outputId": "737f8cd3-bed4-48eb-e2ae-0d437ae6b318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_KEY loaded from Colab secrets.\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
    "    print(\"GROQ_API_KEY loaded from Colab secrets.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a98529",
   "metadata": {
    "id": "83a98529"
   },
   "source": [
    "Before we build our agent, let's understand the foundation: how messages work in LLM systems. Every interaction is structured as messages with specific roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa5541",
   "metadata": {
    "id": "d8fa5541"
   },
   "source": [
    "## Building MSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2af018",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd2af018",
    "outputId": "c93b7d5e-85a4-4f91-c4ba-5115387be334",
    "time_run": "8:04:42p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'Oh Hello There, Myself Sagar'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = \"Oh Hello There, Myself Sagar\"\n",
    "mk_msg(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529cba61",
   "metadata": {
    "id": "529cba61",
    "use_thinking": true
   },
   "source": [
    "Lets go deeper on the above output. Here there are couple of things to keep in mind.\n",
    "1. **Role** : indicate various palyer in the a simple message. Below are fewer\n",
    "    - **user** : message sent by the user, like the message one sent to the chatgpt text box\n",
    "    - **system** : overall the behviour of the given system\n",
    "    - **assistant** : model responses\n",
    "    - **think** : special thinking for the reasoning model\n",
    "    - **tool** : tool call\n",
    "2. **content** : attributes for each field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2f430d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c2f430d",
    "outputId": "2d03ee95-6133-4ae5-b471-d11add0b5456",
    "time_run": "8:04:42p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'role': 'system', 'content': 'You are a helpful math assistant'},\n",
       " {'role': 'user', 'content': 'What is 5 + 3?'},\n",
       " {'role': 'assistant', 'content': 'The answer is 8'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_msg(\"You are a helpful math assistant\", role='system'), mk_msg(\"What is 5 + 3?\", role='user'),mk_msg(\"The answer is 8\", role='assistant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8dda23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c8dda23",
    "outputId": "3d852379-ded9-43a9-e31e-26d886e0ebfc",
    "time_run": "8:04:42p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful math assistant'},\n",
       " {'role': 'user', 'content': 'What is 5 + 3?'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = [\n",
    "    mk_msg(\"You are a helpful math assistant\", role='system'),\n",
    "    mk_msg(\"What is 5 + 3?\", role='user'),\n",
    "    #mk_msg(\"The answer is 8\", role='assistant')\n",
    "]\n",
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c33b1",
   "metadata": {
    "id": "3a5c33b1"
   },
   "source": [
    "## litellm setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ba797a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0ba797a",
    "outputId": "f7662826-3f72-40ff-ac93-a1db6d0b2e88",
    "time_run": "8:04:43p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choices(finish_reason='stop', index=0, message=Message(content='The answer is 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning='The user asks: \"What is 5 + 3?\" It\\'s a simple arithmetic question. The answer is 8. We should respond simply.'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = litellm.completion(\n",
    "    model=model_name,\n",
    "    messages=msg,\n",
    ")\n",
    "r.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b68d075",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b68d075",
    "outputId": "24947d45-ac0b-4423-abe6-02fc23f01ec4",
    "time_run": "8:04:43p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 8.\n",
      "assistant\n"
     ]
    }
   ],
   "source": [
    "print(r.choices[0].message.content)\n",
    "print(r.choices[0].message.role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b2eaf",
   "metadata": {
    "id": "4c2b2eaf"
   },
   "source": [
    "Let's break down the key parts of the response. The `r.choices[0].message` contains:\n",
    "- **content**: '5 + 3 = 8' - This is the actual text response from the model\n",
    "- **role**: 'assistant' - This indicates the message came from the assistant (the LLM)\n",
    "\n",
    "This is exactly the format we need if we wanted to continue the conversation! We have to append this message to msg list, then add another user message, and call the API again. This is known as **conversational context**, the pattern would be :\n",
    "1. Start with your initial messages (system + user)\n",
    "1. Get the response from the LLM\n",
    "1. Append that assistant message to your msg list\n",
    "1. Add a new user message\n",
    "1. Call the API again with the full history\n",
    "This way, the model \"remembers\" what was said before. As you noted, longer context = more tokens = more cost and processing time, but also more coherent conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febfe6a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "febfe6a4",
    "outputId": "7f47d650-9ad8-4127-86ce-c69b8ef89c56",
    "time_run": "8:04:43p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful math assistant'},\n",
       " {'role': 'user', 'content': 'What is 5 + 3?'},\n",
       " {'role': 'assistant', 'content': 'The answer is 8.'},\n",
       " {'role': 'user',\n",
       "  'content': 'can you explain the resoning behind it as I am a 5 year old'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_msg = msg + [\n",
    "    mk_msg(r.choices[0].message.content, role=r.choices[0].message.role),\n",
    "    mk_msg(\"can you explain the resoning behind it as I am a 5 year old\", role=\"user\")\n",
    "]\n",
    "next_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e825731",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "9e825731",
    "outputId": "0307d192-8a52-4921-c663-92c1d8ce2449",
    "time_run": "8:04:43p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Letâ€™s think about it like a fun game with apples.\n",
       "\n",
       "1. **Start with 5 apples.**  \n",
       "   Imagine you have a bowl that already holds 5 shiny apples. Count them:  \n",
       "   1, 2, 3, 4, 5.  \n",
       "   So, we have **5 apples**.\n",
       "\n",
       "2. **Add 3 more apples.**  \n",
       "   Now someone puts 3 extra apples into the same bowl.  \n",
       "   Count the new apples:  \n",
       "   1, 2, 3.  \n",
       "   So, weâ€™re adding **3 apples** to the bowl.\n",
       "\n",
       "3. **Count all the apples together.**  \n",
       "   Put the 5 apples and the 3 new apples in one big group and count everything:  \n",
       "   1, 2, 3, 4, 5, **6, 7, 8**.  \n",
       "   That gives us **8 apples** in total.\n",
       "\n",
       "So, when we do â€œ5 + 3,â€ weâ€™re just putting the 5 things and the 3 things together and seeing how many we have altogether. And that number is 8. Easy peasy!\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-3c23ed54-473f-4729-bb1d-eabf3829de6f`\n",
       "- model: `openai/gpt-oss-20b`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=376, prompt_tokens=120, total_tokens=496, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=126, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.020862679, prompt_time=0.005752552, completion_time=0.370259862, total_time=0.376012414)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-3c23ed54-473f-4729-bb1d-eabf3829de6f', created=1763203547, model='openai/gpt-oss-20b', object='chat.completion', system_fingerprint='fp_e594c51153', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Sure! Letâ€™s think about it like a fun game with apples.\\n\\n1. **Start with 5 apples.**  \\n   Imagine you have a bowl that already holds 5 shiny apples. Count them:  \\n   1, 2, 3, 4, 5.  \\n   So, we have **5 apples**.\\n\\n2. **Add 3 more apples.**  \\n   Now someone puts 3 extra apples into the same bowl.  \\n   Count the new apples:  \\n   1, 2, 3.  \\n   So, weâ€™re adding **3 apples** to the bowl.\\n\\n3. **Count all the apples together.**  \\n   Put the 5 apples and the 3 new apples in one big group and count everything:  \\n   1, 2, 3, 4, 5, **6, 7, 8**.  \\n   That gives us **8 apples** in total.\\n\\nSo, when we do â€œ5 + 3,â€ weâ€™re just putting the 5 things and the 3 things together and seeing how many we have altogether. And that number is 8. Easy peasy!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning='User wants explanation for 5+3 for a 5-year-old. So we need a simple, child-friendly explanation: adding numbers, counting, etc. Use simple language, maybe counting with fingers. Explain that 5 plus 3 is like having five apples and then adding three more apples, you have eight apples total. Use simple analogies.\\n\\nWe must be mindful of content guidelines: no disallowed content. This is safe. Should not mention policy.\\n\\nWe should comply with user request. Provide explanation. Use simple language. Possibly include visual or analogy. Should be short and easy for a 5-year-old.'))], usage=Usage(completion_tokens=376, prompt_tokens=120, total_tokens=496, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=126, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.020862679, prompt_time=0.005752552, completion_time=0.370259862, total_time=0.376012414), usage_breakdown=None, x_groq={'id': 'req_01ka3hy0a0fq7v71pgx9y38z8h'}, service_tier='auto')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = litellm.completion(\n",
    "    model=model_name,\n",
    "    messages=next_msg,\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27dbb6",
   "metadata": {
    "id": "5b27dbb6"
   },
   "source": [
    "Now that we understand how LLMs handle conversations, let's put this to work. We'll build an agent that can actually do things - specifically, search files using command-line tools. This demonstrates how agents combine reasoning (LLM) with actions (tool execution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5ec7a",
   "metadata": {
    "id": "dde5ec7a",
    "use_thinking": true
   },
   "source": [
    "\n",
    "\n",
    "## Building a Safe File Search Agent\n",
    "\n",
    "An **agent** in the LLM context is a system that can take actions beyond just generating text. While a standard LLM can reason and respond, an agent can execute commands, use tools, and interact with external systems.\n",
    "\n",
    "In this section, we're building a **file search agent** that can help users find and search through files using common bash commands like `find`, `grep`, and `ls`. This is a practical example because:\n",
    "\n",
    "1. **Real-world utility**: File searching is a common task that benefits from natural language interfaces\n",
    "2. **Safety-first design**: We'll implement strict safeguards to ensure read-only operations\n",
    "3. **Multi-tool coordination**: The LLM must decide which tools to use and in what order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe78fbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7fe78fbd",
    "outputId": "b10bb8e3-34a3-4a8b-9454-9ea33854d0f1",
    "time_run": "8:04:44p"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"265pt\" height=\"771pt\"\n",
       " viewBox=\"0.00 0.00 265.00 771.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 767)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-767 261,-767 261,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"lightblue\" stroke=\"black\" d=\"M245,-763C245,-763 12,-763 12,-763 6,-763 0,-757 0,-751 0,-751 0,-737 0,-737 0,-731 6,-725 12,-725 12,-725 245,-725 245,-725 251,-725 257,-731 257,-737 257,-737 257,-751 257,-751 257,-757 251,-763 245,-763\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-747.8\" font-family=\"Times,serif\" font-size=\"14.00\">User asks question</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-732.8\" font-family=\"Times,serif\" font-size=\"14.00\">&quot;Find all Python files with &#39;litellm&#39; in them&quot;</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"lightblue\" stroke=\"black\" d=\"M185.5,-689C185.5,-689 71.5,-689 71.5,-689 65.5,-689 59.5,-683 59.5,-677 59.5,-677 59.5,-603 59.5,-603 59.5,-597 65.5,-591 71.5,-591 71.5,-591 185.5,-591 185.5,-591 191.5,-591 197.5,-597 197.5,-603 197.5,-603 197.5,-677 197.5,-677 197.5,-683 191.5,-689 185.5,-689\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-673.8\" font-family=\"Times,serif\" font-size=\"14.00\">Send to LLM with:</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45; Conversation history</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-643.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45; Available tools:</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-628.8\" font-family=\"Times,serif\" font-size=\"14.00\"> &#160;* find_files</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-613.8\" font-family=\"Times,serif\" font-size=\"14.00\"> &#160;* grep_files</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-598.8\" font-family=\"Times,serif\" font-size=\"14.00\"> &#160;* list_directory</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-724.76C128.5,-717.44 128.5,-708.56 128.5,-699.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-699.28 128.5,-689.28 125,-699.28 132,-699.28\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"lightyellow\" stroke=\"black\" d=\"M206,-555C206,-555 51,-555 51,-555 45,-555 39,-549 39,-543 39,-543 39,-514 39,-514 39,-508 45,-502 51,-502 51,-502 206,-502 206,-502 212,-502 218,-508 218,-514 218,-514 218,-543 218,-543 218,-549 212,-555 206,-555\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-539.8\" font-family=\"Times,serif\" font-size=\"14.00\">LLM decides:</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-524.8\" font-family=\"Times,serif\" font-size=\"14.00\">&quot;I need to use find_files first&quot;</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-509.8\" font-family=\"Times,serif\" font-size=\"14.00\">Returns: tool_call request</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-590.97C128.5,-582.42 128.5,-573.65 128.5,-565.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-565.23 128.5,-555.23 125,-565.23 132,-565.23\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"lightgreen\" stroke=\"black\" d=\"M207.5,-466C207.5,-466 49.5,-466 49.5,-466 43.5,-466 37.5,-460 37.5,-454 37.5,-454 37.5,-425 37.5,-425 37.5,-419 43.5,-413 49.5,-413 49.5,-413 207.5,-413 207.5,-413 213.5,-413 219.5,-419 219.5,-425 219.5,-425 219.5,-454 219.5,-454 219.5,-460 213.5,-466 207.5,-466\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-450.8\" font-family=\"Times,serif\" font-size=\"14.00\">Execute tool:</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-435.8\" font-family=\"Times,serif\" font-size=\"14.00\">find_files(&quot;temp_dir&quot;, &quot;*.py&quot;)</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-420.8\" font-family=\"Times,serif\" font-size=\"14.00\">â†’ Returns file list</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-501.87C128.5,-493.89 128.5,-484.93 128.5,-476.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-476.25 128.5,-466.25 125,-476.25 132,-476.25\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"lightblue\" stroke=\"black\" d=\"M193.5,-377C193.5,-377 63.5,-377 63.5,-377 57.5,-377 51.5,-371 51.5,-365 51.5,-365 51.5,-351 51.5,-351 51.5,-345 57.5,-339 63.5,-339 63.5,-339 193.5,-339 193.5,-339 199.5,-339 205.5,-345 205.5,-351 205.5,-351 205.5,-365 205.5,-365 205.5,-371 199.5,-377 193.5,-377\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-361.8\" font-family=\"Times,serif\" font-size=\"14.00\">Send result back to LLM</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-346.8\" font-family=\"Times,serif\" font-size=\"14.00\">(as &quot;tool&quot; role message)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-412.91C128.5,-404.74 128.5,-395.65 128.5,-387.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-387.24 128.5,-377.24 125,-387.24 132,-387.24\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"lightyellow\" stroke=\"black\" d=\"M215.5,-303C215.5,-303 41.5,-303 41.5,-303 35.5,-303 29.5,-297 29.5,-291 29.5,-291 29.5,-262 29.5,-262 29.5,-256 35.5,-250 41.5,-250 41.5,-250 215.5,-250 215.5,-250 221.5,-250 227.5,-256 227.5,-262 227.5,-262 227.5,-291 227.5,-291 227.5,-297 221.5,-303 215.5,-303\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-287.8\" font-family=\"Times,serif\" font-size=\"14.00\">LLM decides:</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-272.8\" font-family=\"Times,serif\" font-size=\"14.00\">&quot;Now grep each file&quot;</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-257.8\" font-family=\"Times,serif\" font-size=\"14.00\">Returns: multiple grep_files calls</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-338.96C128.5,-331.32 128.5,-322.14 128.5,-313.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-313 128.5,-303 125,-313 132,-313\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"lightgreen\" stroke=\"black\" d=\"M211,-214C211,-214 46,-214 46,-214 40,-214 34,-208 34,-202 34,-202 34,-158 34,-158 34,-152 40,-146 46,-146 46,-146 211,-146 211,-146 217,-146 223,-152 223,-158 223,-158 223,-202 223,-202 223,-208 217,-214 211,-214\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-198.8\" font-family=\"Times,serif\" font-size=\"14.00\">Execute tools:</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-183.8\" font-family=\"Times,serif\" font-size=\"14.00\">grep_files(&quot;litellm&quot;, &quot;file1.py&quot;)</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-168.8\" font-family=\"Times,serif\" font-size=\"14.00\">grep_files(&quot;litellm&quot;, &quot;file2.py&quot;)</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-153.8\" font-family=\"Times,serif\" font-size=\"14.00\">etc.</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-249.68C128.5,-241.82 128.5,-232.96 128.5,-224.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-224.25 128.5,-214.25 125,-224.25 132,-224.25\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"lightblue\" stroke=\"black\" d=\"M181,-110C181,-110 76,-110 76,-110 70,-110 64,-104 64,-98 64,-98 64,-86 64,-86 64,-80 70,-74 76,-74 76,-74 181,-74 181,-74 187,-74 193,-80 193,-86 193,-86 193,-98 193,-98 193,-104 187,-110 181,-110\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-88.3\" font-family=\"Times,serif\" font-size=\"14.00\">Send all results back</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-146C128.5,-137.56 128.5,-128.59 128.5,-120.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-120.4 128.5,-110.4 125,-120.4 132,-120.4\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"lightcoral\" stroke=\"black\" d=\"M221.5,-38C221.5,-38 35.5,-38 35.5,-38 29.5,-38 23.5,-32 23.5,-26 23.5,-26 23.5,-12 23.5,-12 23.5,-6 29.5,0 35.5,0 35.5,0 221.5,0 221.5,0 227.5,0 233.5,-6 233.5,-12 233.5,-12 233.5,-26 233.5,-26 233.5,-32 227.5,-38 221.5,-38\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">LLM generates final response:</text>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">&quot;Found litellm in: file2.py (line 2)&quot;</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.5,-73.81C128.5,-66.11 128.5,-56.82 128.5,-48.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132,-48.02 128.5,-38.02 125,-48.02 132,-48.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ee9d9bc3590>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "def agent_flow_chart():\n",
    "    dot = Digraph(comment='Agent File Search Flow')\n",
    "    dot.attr(rankdir='TB')\n",
    "    dot.attr('node', shape='box', style='rounded,filled', fillcolor='lightblue')\n",
    "\n",
    "    # Add nodes\n",
    "    dot.node('1', 'User asks question\\n\"Find all Python files with \\'litellm\\' in them\"')\n",
    "    dot.node('2', 'Send to LLM with:\\n- Conversation history\\n- Available tools:\\n  * find_files\\n  * grep_files\\n  * list_directory')\n",
    "    dot.node('3', 'LLM decides:\\n\"I need to use find_files first\"\\nReturns: tool_call request', fillcolor='lightyellow')\n",
    "    dot.node('4', 'Execute tool:\\nfind_files(\"temp_dir\", \"*.py\")\\nâ†’ Returns file list', fillcolor='lightgreen')\n",
    "    dot.node('5', 'Send result back to LLM\\n(as \"tool\" role message)')\n",
    "    dot.node('6', 'LLM decides:\\n\"Now grep each file\"\\nReturns: multiple grep_files calls', fillcolor='lightyellow')\n",
    "    dot.node('7', 'Execute tools:\\ngrep_files(\"litellm\", \"file1.py\")\\ngrep_files(\"litellm\", \"file2.py\")\\netc.', fillcolor='lightgreen')\n",
    "    dot.node('8', 'Send all results back')\n",
    "    dot.node('9', 'LLM generates final response:\\n\"Found litellm in: file2.py (line 2)\"', fillcolor='lightcoral')\n",
    "\n",
    "    # Add edges\n",
    "    dot.edge('1', '2')\n",
    "    dot.edge('2', '3')\n",
    "    dot.edge('3', '4')\n",
    "    dot.edge('4', '5')\n",
    "    dot.edge('5', '6')\n",
    "    dot.edge('6', '7')\n",
    "    dot.edge('7', '8')\n",
    "    dot.edge('8', '9')\n",
    "\n",
    "    return dot\n",
    "\n",
    "agent_flow_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf6d952",
   "metadata": {
    "id": "bbf6d952",
    "time_run": "8:04:44p"
   },
   "outputs": [],
   "source": [
    "from toolslm.funccall import get_schema\n",
    "import json\n",
    "from fastcore.xtras import run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8f117",
   "metadata": {
    "id": "48b8f117"
   },
   "source": [
    "### Mock Setup for File Search\n",
    "\n",
    "Created `temp_dir` and some dummy files to perform agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6491461",
   "metadata": {
    "id": "a6491461",
    "time_run": "8:04:44p",
    "use_thinking": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def test_setup():\n",
    "    # Clean up and create directories\n",
    "    os.system('rm -rf temp_dir')\n",
    "    os.makedirs('temp_dir/subdir', exist_ok=True)\n",
    "\n",
    "    # Create file1.py\n",
    "    with open('temp_dir/file1.py', 'w') as f:\n",
    "        f.write('''import numpy as np\n",
    "from fastcore.xtras import run\n",
    "\n",
    "def safe_run(cmd):\n",
    "    return run(cmd)\n",
    "\n",
    "def helper():\n",
    "    pass\n",
    "''')\n",
    "\n",
    "    # Create file2.py\n",
    "    with open('temp_dir/file2.py', 'w') as f:\n",
    "        f.write('''import pandas as pd\n",
    "import litellm\n",
    "\n",
    "def process_data():\n",
    "    return \"data\"\n",
    "''')\n",
    "\n",
    "    # Create file3.py\n",
    "    with open('temp_dir/subdir/file3.py', 'w') as f:\n",
    "        f.write('''from fastcore.script import call_parse\n",
    "\n",
    "def deep_function():\n",
    "    pass\n",
    "''')\n",
    "\n",
    "    # Create test.ipynb\n",
    "    with open('temp_dir/test.ipynb', 'w') as f:\n",
    "        f.write('''{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\"import litellm\\\\nprint('hello')\"]\n",
    "  }\n",
    " ]\n",
    "}\n",
    "''')\n",
    "\n",
    "    # Create another.ipynb\n",
    "    with open('temp_dir/subdir/another.ipynb', 'w') as f:\n",
    "        f.write('''{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\"def safe_run():\\\\n    pass\"]\n",
    "  }\n",
    " ]\n",
    "}\n",
    "''')\n",
    "\n",
    "    # Verify\n",
    "    os.system('find temp_dir -type f')\n",
    "\n",
    "test_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e577c2",
   "metadata": {
    "id": "80e577c2"
   },
   "source": [
    "### Safe Run Safety Considerations\n",
    "The flowchart shows our target architecture. But before implementing the tools, we must address a critical concern: security. Since our agent executes shell commands, we need strict safeguards. Our agent will perform only read operation no delete, update or execute.\n",
    "- **Whitelist commands only**: Only `find`, `grep`, and `ls` are allowed\n",
    "- **Block dangerous characters**: No pipes (`|`), semicolons (`;`), or command substitution\n",
    "- **Read-only operations**: No `rm`, `mv`, `cp`, or file modification commands\n",
    "- **Structured arguments**: Using command lists instead of shell strings to prevent injection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1465a699",
   "metadata": {
    "id": "1465a699",
    "time_run": "8:04:44p"
   },
   "outputs": [],
   "source": [
    "ALLOWED_COMMANDS = {'find', 'grep', 'ls'}\n",
    "DANGEROUS_CHARS = {'|', ';', '&', '>', '<', '`', '$', '(', ')'}\n",
    "\n",
    "def safe_run(cmd_list):\n",
    "    \"\"\"Safely execute only whitelisted commands\"\"\"\n",
    "    if not cmd_list:\n",
    "        raise ValueError(\"Empty command list\")\n",
    "\n",
    "    if cmd_list[0] not in ALLOWED_COMMANDS:\n",
    "        raise ValueError(f\"Command '{cmd_list[0]}' not allowed. Only {ALLOWED_COMMANDS} permitted.\")\n",
    "\n",
    "    # Check for shell operators\n",
    "    for item in cmd_list:\n",
    "        if any(char in str(item) for char in DANGEROUS_CHARS):\n",
    "            raise ValueError(f\"Dangerous characters detected in command\")\n",
    "\n",
    "    return run(cmd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf13655c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf13655c",
    "outputId": "ed12b79c-355c-4d19-f855-34f7b0909414",
    "time_run": "8:04:44p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASSED - caught error: Command 'rm' not allowed. Only {'find', 'ls', 'grep'} permitted.\n",
      "PASSED - caught error: Dangerous characters detected in command\n",
      "PASSED - caught error: Dangerous characters detected in command\n",
      "PASSED - caught error: Dangerous characters detected in command\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Using try/except\n",
    "dangerous_tests = [\n",
    "    ['rm', 'file'],           # Not in whitelist\n",
    "    ['find', '.', ';', 'rm'], # Semicolon\n",
    "    ['grep', 'test', '&'],    # Ampersand\n",
    "    ['ls', '-lart', '|', 'wc']\n",
    "]\n",
    "\n",
    "for c in dangerous_tests:\n",
    "    try:\n",
    "        safe_run(c)\n",
    "        print(\"FAILED - should have raised error\")\n",
    "    except ValueError as e:\n",
    "        print(f\"PASSED - caught error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f05660b9",
   "metadata": {
    "id": "f05660b9",
    "time_run": "8:04:44p"
   },
   "outputs": [],
   "source": [
    "assert safe_run(['find', '.', '-name', '*.ipynb']) != '', \"find should return output\"\n",
    "assert type(safe_run(['ls', '-la'])) == str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb1a21",
   "metadata": {
    "id": "dfbb1a21"
   },
   "source": [
    "With safety established, let's build the actual tools our agent will use. Each tool wraps a command-line utility with a clean Python interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f773a",
   "metadata": {
    "id": "121f773a"
   },
   "source": [
    "### Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51787ebd",
   "metadata": {
    "id": "51787ebd",
    "time_run": "8:04:44p"
   },
   "outputs": [],
   "source": [
    "def find_files(\n",
    "    directory: str,      # Starting directory (e.g., \".\", \"/home/user\")\n",
    "    name: str = \"*\",     # Filename pattern (e.g., \"*.py\", \"test*\")\n",
    "    file_type: str = '', # File type: \"f\" (file), \"d\" (dir), or None (any)\n",
    "    maxdepth: int = -1   # Limit search depth for safety\n",
    ") -> str:\n",
    "    \"\"\"Find files matching criteria\"\"\"\n",
    "    cmd = [\"find\", directory]\n",
    "\n",
    "    if maxdepth != -1:\n",
    "        cmd += ['-maxdepth', str(maxdepth)]\n",
    "\n",
    "    if file_type != '':\n",
    "        cmd += ['-type', file_type]\n",
    "\n",
    "    cmd += [\"-name\", name]\n",
    "\n",
    "    try:\n",
    "        return safe_run(cmd)\n",
    "    except (IOError, OSError) as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1dbe2dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1dbe2dc",
    "outputId": "b972286b-b652-4f2e-d62f-1a62ab9fcf2b",
    "time_run": "8:04:44p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_dir/file2.py\n",
      "temp_dir/file1.py\n",
      "temp_dir/subdir/file3.py\n"
     ]
    }
   ],
   "source": [
    "print(find_files('temp_dir', '*.py', maxdepth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23ab3b",
   "metadata": {
    "id": "1b23ab3b"
   },
   "source": [
    "### Grep files\n",
    "\n",
    "**For `grep_files` (grep):**\n",
    "- `pattern: str` - What to search for\n",
    "- `file_path: str` - Which file to search in\n",
    "- `ignore_case: bool = False` - Case-insensitive? (translates to `-i`)\n",
    "- `line_numbers: bool = False` - Show line numbers? (translates to `-n`)\n",
    "- `show_filename: bool = True` - filenames to always appear for context (`-H` - Always show filename/`-h` - Never show filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0da6f79",
   "metadata": {
    "id": "e0da6f79",
    "time_run": "8:04:45p"
   },
   "outputs": [],
   "source": [
    "def grep_files(\n",
    "    pattern: str,                 #  Pattern to search for\n",
    "    file_path: str,               # Single file to search in\n",
    "    ignore_case: bool = False,    # Case-insensitive search (-i)\n",
    "    line_numbers: bool = False,   # Show line numbers (-n)\n",
    "    show_filename: bool = True    # Always show filename (-H)\n",
    ") -> str:\n",
    "    \"\"\"Search for pattern in a single file using grep\n",
    "\n",
    "    For searching multiple files, first use find_files to get the list,\n",
    "    then call grep_files on each file separately.\n",
    "\n",
    "    Args:\n",
    "        pattern: Text pattern to search for\n",
    "        file_path: Path to a single file to search (not wildcards)\n",
    "        ignore_case: If True, ignore case when matching\n",
    "        line_numbers: If True, show line numbers in output\n",
    "        show_filename: If True, always show filename in output\n",
    "\n",
    "    Returns:\n",
    "        Grep output showing matching lines\n",
    "    \"\"\"\n",
    "    cmd = [\"grep\"]\n",
    "\n",
    "    if ignore_case:\n",
    "        cmd.append('-i')\n",
    "\n",
    "    if line_numbers:\n",
    "        cmd.append('-n')\n",
    "\n",
    "    if show_filename:\n",
    "        cmd.append('-H')\n",
    "    else:\n",
    "        cmd.append('-h')\n",
    "\n",
    "    cmd += [pattern, file_path]\n",
    "\n",
    "    try:\n",
    "        return safe_run(cmd)\n",
    "    except (IOError, OSError):\n",
    "        # grep returns exit code 1 when no matches found\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "529538f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "529538f5",
    "outputId": "e2fd1233-bfa8-4055-ccc2-f100a0159e89",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_dir/file2.py:1:import pandas as pd\n",
      "temp_dir/file2.py:2:import litellm\n"
     ]
    }
   ],
   "source": [
    "print(grep_files('import ', 'temp_dir/file2.py', line_numbers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb6e863a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb6e863a",
    "outputId": "e1422a4b-78fe-431e-b9d6-71b1daa06a45",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(grep_files('import ', 'temp_dir/*', line_numbers=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb0e6a",
   "metadata": {
    "id": "f9bb0e6a"
   },
   "source": [
    "### List Directory\n",
    "**For `list_directory` (ls):**\n",
    "- `directory: str` - Which directory to list\n",
    "- `show_hidden: bool = False` - Include hidden files? (translates to `-a`)\n",
    "- `long_format: bool = False` - Detailed listing? (translates to `-l`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92c8800b",
   "metadata": {
    "id": "92c8800b",
    "time_run": "8:04:45p"
   },
   "outputs": [],
   "source": [
    "\n",
    "def list_directory(\n",
    "    directory: str,\n",
    "    show_hidden: bool = False,\n",
    "    long_format: bool = False,\n",
    ") -> str :\n",
    "    \"\"\"List directory given a directory\"\"\"\n",
    "    cmd = [\"ls\"]\n",
    "    if show_hidden: cmd.append('-a')\n",
    "    if long_format: cmd.append('-l')\n",
    "    cmd.append(directory)\n",
    "\n",
    "    try:\n",
    "        return safe_run(cmd)\n",
    "    except (IOError, OSError) as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88058d54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88058d54",
    "outputId": "be99bda1-14a2-4711-cb58-e11d9d242d90",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1.py\n",
      "file2.py\n",
      "subdir\n",
      "test.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(list_directory('temp_dir'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348803f7",
   "metadata": {
    "id": "348803f7"
   },
   "source": [
    "We have our safe tools ready. Now comes the key part: teaching the LLM about these tools so it can decide when and how to use them. This is where tool schemas come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84449d1",
   "metadata": {
    "id": "a84449d1"
   },
   "source": [
    "### Basic Tool Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e6b0e",
   "metadata": {
    "id": "789e6b0e"
   },
   "source": [
    "\n",
    "**Tools are functions** - Like `find_files`, `grep_files` and `list_directory` functions that perform actual actions.\n",
    "\n",
    "**The model needs to know about them** - We must describe these tools to the LLM using a schema (name, description, parameters) so it knows:\n",
    "- What tools are available\n",
    "- What each tool does\n",
    "- What inputs each tool expects\n",
    "\n",
    "**This prevents confusion** - Without clear descriptions, the LLM wouldn't know it can call these tools or how to use them properly.\n",
    "\n",
    "We will use, `toolslm` uses to generate the schema automatically! The structure required for `Groq/OpenAI expects`.\n",
    "\n",
    "**Note**: If you are using any thing other than the model `groq/openai/gpt-oss-20b` you have to tweek the tool format required for that model.\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"list_directory\",\n",
    "      \"description\": \"List directory given a directory\\n\\nReturns:\\n- type: string\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"directory\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"\"\n",
    "          },\n",
    "          \"show_hidden\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"\",\n",
    "            \"default\": false\n",
    "          },\n",
    "          \"long_format\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"\",\n",
    "            \"default\": false\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"directory\"]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4321dfb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4321dfb2",
    "outputId": "eda605e5-81ad-4c61-d9c5-de22ec91a464",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'list_directory',\n",
       "   'description': 'List directory given a directory\\n\\nReturns:\\n- type: string',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'directory': {'type': 'string', 'description': ''},\n",
       "     'show_hidden': {'type': 'boolean', 'description': '', 'default': False},\n",
       "     'long_format': {'type': 'boolean', 'description': '', 'default': False}},\n",
       "    'required': ['directory']}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'grep_files',\n",
       "   'description': 'Search for pattern in a single file using grep\\n\\n    For searching multiple files, first use find_files to get the list,\\n    then call grep_files on each file separately.\\n\\n    Args:\\n        pattern: Text pattern to search for\\n        file_path: Path to a single file to search (not wildcards)\\n        ignore_case: If True, ignore case when matching\\n        line_numbers: If True, show line numbers in output\\n        show_filename: If True, always show filename in output\\n\\n    Returns:\\n        Grep output showing matching lines\\n    \\n\\nReturns:\\n- type: string',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'pattern': {'type': 'string',\n",
       "      'description': 'Pattern to search for'},\n",
       "     'file_path': {'type': 'string',\n",
       "      'description': 'Single file to search in'},\n",
       "     'ignore_case': {'type': 'boolean',\n",
       "      'description': 'Case-insensitive search (-i)',\n",
       "      'default': False},\n",
       "     'line_numbers': {'type': 'boolean',\n",
       "      'description': 'Show line numbers (-n)',\n",
       "      'default': False},\n",
       "     'show_filename': {'type': 'boolean',\n",
       "      'description': 'Always show filename (-H)',\n",
       "      'default': True}},\n",
       "    'required': ['pattern', 'file_path']}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'find_files',\n",
       "   'description': 'Find files matching criteria\\n\\nReturns:\\n- type: string',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'directory': {'type': 'string',\n",
       "      'description': 'Starting directory (e.g., \".\", \"/home/user\")'},\n",
       "     'name': {'type': 'string',\n",
       "      'description': 'Filename pattern (e.g., \"*.py\", \"test*\")',\n",
       "      'default': '*'},\n",
       "     'file_type': {'type': 'string',\n",
       "      'description': 'File type: \"f\" (file), \"d\" (dir), or None (any)',\n",
       "      'default': ''},\n",
       "     'maxdepth': {'type': 'integer',\n",
       "      'description': 'Limit search depth for safety',\n",
       "      'default': -1}},\n",
       "    'required': ['directory']}}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_tool(func):\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": get_schema(func, pname='parameters')\n",
    "         }\n",
    "tools = [build_tool(i) for i in (list_directory, grep_files, find_files)]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be122335",
   "metadata": {
    "id": "be122335"
   },
   "source": [
    "Since we are using a reasoning model that can do arithmetic on its own, need to **force** it to use the calculator tools.\n",
    "\n",
    "Here's what the `tool_choice` parameter does:\n",
    "- **`None`** (default): Model won't use tools at all\n",
    "- **`\"auto\"`**: Model decides whether to use tools (might skip them for simple math)\n",
    "- **`\"required\"`**: Model MUST use one of the provided tools\n",
    "\n",
    "Note : After using required, the model we are using is a Reasoning Model. There is a chance it can do the simple arithmetic without waiting for tool use stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b380289",
   "metadata": {},
   "source": [
    "### Tool Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaecd59",
   "metadata": {
    "id": "cfaecd59",
    "time_run": "8:04:45p"
   },
   "outputs": [],
   "source": [
    "# llm call\n",
    "def invoke(pr):\n",
    "    return litellm.completion(\n",
    "        model=model_name,\n",
    "        messages=pr,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8ac95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56c8ac95",
    "outputId": "5f6f82f8-6d2c-4a87-b4dd-4855927221e3",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.'},\n",
       " {'role': 'user', 'content': 'list all the files in the current directory.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic message setup\n",
    "def get_msg(msg):\n",
    "    return [\n",
    "        mk_msg(role=\"system\", content=\"You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.\"),\n",
    "        mk_msg(role=\"user\", content=msg,)\n",
    "    ]\n",
    "\n",
    "pr = get_msg('list all the files in the current directory.')\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0cf35e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "b0cf35e3",
    "outputId": "822faf41-03ce-4897-945f-717e641e3bb1",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ğŸ”§ list_directory({\"directory\":\".\"})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-a6f7fc62-0ea0-4316-b348-9d4dfa6ee025`\n",
       "- model: `openai/gpt-oss-20b`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=35, prompt_tokens=532, total_tokens=567, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=12, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.01802854, prompt_time=0.02795786, completion_time=0.034892549, total_time=0.062850409)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-a6f7fc62-0ea0-4316-b348-9d4dfa6ee025', created=1763203548, model='openai/gpt-oss-20b', object='chat.completion', system_fingerprint='fp_5979a0e1b7', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"directory\":\".\"}', name='list_directory'), id='fc_a63bcd76-0a4c-4a1a-a880-894fbcd464db', type='function')], function_call=None, provider_specific_fields=None, reasoning='We need to use list_directory tool. Provide results.'))], usage=Usage(completion_tokens=35, prompt_tokens=532, total_tokens=567, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=12, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.01802854, prompt_time=0.02795786, completion_time=0.034892549, total_time=0.062850409), usage_breakdown=None, x_groq={'id': 'req_01ka3hy15dfqcscnbwya64tb6a'}, service_tier='auto')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = get_msg('list all the files in the current directory.')\n",
    "res = invoke(pr)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f099611",
   "metadata": {
    "id": "1f099611",
    "time_run": "8:04:45p"
   },
   "outputs": [],
   "source": [
    "# to check if the resp have the tool_calls\n",
    "def has_tools(res):\n",
    "    return hasattr(res.choices[0].message, 'tool_calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6443771",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6443771",
    "outputId": "5ea0338b-2fa2-4c19-8bf1-4d12e6886bc5",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_tools(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a7d5c",
   "metadata": {},
   "source": [
    "Extract the function from the tool object of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "745dfde5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "745dfde5",
    "outputId": "611ddfaa-fa48-4b7b-8f80-e15967146064",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>list_directory</b><br/>def list_directory(directory: str, show_hidden: bool=False, long_format: bool=False) -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-2365635016.py</a>List directory given a directory</pre></div>"
      ],
      "text/plain": [
       "<function __main__.list_directory(directory: str, show_hidden: bool = False, long_format: bool = False) -> str>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def res2func_nm(res):\n",
    "    if hasattr(res.choices[0].message.tool_calls[0], 'function'):\n",
    "        func_name  = res.choices[0].message.tool_calls[0].function.name\n",
    "        func = globals().get(func_name, None)\n",
    "        return func\n",
    "\n",
    "    return False\n",
    "\n",
    "res2func_nm(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f211f3",
   "metadata": {},
   "source": [
    "Extract the function kwargs from the tool object of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af4be61c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af4be61c",
    "outputId": "62d4830d-5351-4135-e814-10a47c536adc",
    "time_run": "8:04:45p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directory': '.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_res2func_kwargs(res):\n",
    "    if hasattr(res.choices[0].message.tool_calls[0], 'function'):\n",
    "        func_kwargs = res.choices[0].message.tool_calls[0].function.arguments\n",
    "        func_kwargs = json.loads(func_kwargs)\n",
    "        return func_kwargs\n",
    "\n",
    "    return False\n",
    "\n",
    "get_res2func_kwargs(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e7cd6",
   "metadata": {
    "id": "503e7cd6"
   },
   "source": [
    "Now we need to add two things to your message history:\n",
    "\n",
    "1. **The assistant's message** (with the tool_calls) - so the LLM knows what it asked for\n",
    "2. **The tool result** - as a message with `role='tool'`\n",
    "\n",
    "The assistant's message:\n",
    "- `\"role\"` : `'assistant'`\n",
    "- `\"content\"` : `\"\"`\n",
    "- `\"tool_calls\"` : `res.choices[0].message.tool_calls`\n",
    "\n",
    "The tool result message needs:\n",
    "- `role`: `'tool'`\n",
    "- `tool_call_id`: The ID from the tool call\n",
    "- `content`: The result as a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f4d73",
   "metadata": {
    "id": "3d0f4d73",
    "time_run": "8:04:46p"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_next_call(res):\n",
    "    \"\"\"A helper function to extract new messages with tools and assitant prompt.\"\"\"\n",
    "    func = res2func_nm(res)\n",
    "    func_kwargs = get_res2func_kwargs(res)\n",
    "\n",
    "    t_msg = mk_msg({\n",
    "        \"role\":'tool',\n",
    "        'tool_call_id':res.choices[0].message.tool_calls[0].id,\n",
    "        'content':str(func(**func_kwargs))\n",
    "    })\n",
    "    a_msg= mk_msg({\n",
    "        \"role\":'assistant',\n",
    "        \"content\": \"\",\n",
    "        'tool_calls': res.choices[0].message.tool_calls\n",
    "    })\n",
    "    return [ a_msg, t_msg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002cac2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "002cac2b",
    "outputId": "c6805fd9-c192-4129-a8e6-1295b95e58ac",
    "time_run": "8:04:46p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.'},\n",
       " {'role': 'user', 'content': 'list all the files in the current directory.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '',\n",
       "  'tool_calls': [ChatCompletionMessageToolCall(function=Function(arguments='{\"directory\":\".\"}', name='list_directory'), id='fc_a63bcd76-0a4c-4a1a-a880-894fbcd464db', type='function')]},\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'fc_a63bcd76-0a4c-4a1a-a880-894fbcd464db',\n",
       "  'content': 'sample_data\\ntemp_dir'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def next_msg(pr, res):\n",
    "    \"\"\"extracting and appending the new prompts from `res` i.e for tool and assiant to the original prompt `pr` \"\"\"\n",
    "    return pr + build_next_call(res)\n",
    "\n",
    "pr_ = next_msg(pr, res)\n",
    "pr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46ad0dc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "46ad0dc8",
    "outputId": "9659170a-37ae-45d6-8cfa-87c97838715d",
    "time_run": "8:04:46p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ğŸ”§ list_directory({\"directory\":\".\"})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-950fe2b9-7e08-424d-ba34-183d566f7aa0`\n",
       "- model: `openai/gpt-oss-20b`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=31, prompt_tokens=532, total_tokens=563, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=8, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.017648798, prompt_time=0.030200318, completion_time=0.030406804, total_time=0.060607122)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-950fe2b9-7e08-424d-ba34-183d566f7aa0', created=1763203548, model='openai/gpt-oss-20b', object='chat.completion', system_fingerprint='fp_8b41efc9a3', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"directory\":\".\"}', name='list_directory'), id='fc_97b94ce4-7451-47ab-af01-965969e90e0d', type='function')], function_call=None, provider_specific_fields=None, reasoning='We need to call list_directory.'))], usage=Usage(completion_tokens=31, prompt_tokens=532, total_tokens=563, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=8, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.017648798, prompt_time=0.030200318, completion_time=0.030406804, total_time=0.060607122), usage_breakdown=None, x_groq={'id': 'req_01ka3hy1ckf1d844wn3vm1bxs9'}, service_tier='auto')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = get_msg('list all the files in the current directory.')\n",
    "res = invoke(pr)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db0abb45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db0abb45",
    "outputId": "644395d0-d5c8-4090-cdc4-613959b158bc",
    "time_run": "8:04:46p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.list_directory(directory: str, show_hidden: bool = False, long_format: bool = False) -> str>,\n",
       " {'directory': '.'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if has_tools(res):\n",
    "    func_nm, func_kwargs = res2func_nm(res), get_res2func_kwargs(res)\n",
    "func_nm, func_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2913890f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "2913890f",
    "outputId": "62f48aa6-2d4c-4077-bb7f-bddd447bf840",
    "time_run": "8:04:48p"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The current directory contains:\n",
       "\n",
       "- `sample_data`\n",
       "- `temp_dir`\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-2925729f-ea58-4708-8977-406fd029e125`\n",
       "- model: `openai/gpt-oss-20b`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=53, prompt_tokens=561, total_tokens=614, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=29, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.020229679, prompt_time=0.029933307, completion_time=0.051773638, total_time=0.081706945)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-2925729f-ea58-4708-8977-406fd029e125', created=1763203548, model='openai/gpt-oss-20b', object='chat.completion', system_fingerprint='fp_37c9245f64', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The current directory contains:\\n\\n- `sample_data`\\n- `temp_dir`', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning=\"The tool returned two entries: 'sample_data' and 'temp_dir'. So list_directory shows two entries. Need to output them clearly.\"))], usage=Usage(completion_tokens=53, prompt_tokens=561, total_tokens=614, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=29, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None, queue_time=0.020229679, prompt_time=0.029933307, completion_time=0.051773638, total_time=0.081706945), usage_breakdown=None, x_groq={'id': 'req_01ka3hy1hzf1dazb1rrx2h0qrf'}, service_tier='auto')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ = invoke(pr_)\n",
    "res_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0129f55",
   "metadata": {
    "id": "c0129f55"
   },
   "source": [
    "The model returns `finish_reason` as `stop` for stopping the exexcution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e0962ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5e0962ba",
    "outputId": "5d0128e4-6f9e-4c78-fb2e-7ae2448567b6",
    "time_run": "8:04:49p"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6d25a",
   "metadata": {
    "id": "cdf6d25a"
   },
   "source": [
    "The LLM now knows our tools exist. The final piece is the agent loop: repeatedly calling the LLM, executing its requested tools, and feeding back results until the task completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d7dc77d",
   "metadata": {
    "id": "5d7dc77d",
    "time_run": "8:04:49p"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def tool_loop(pr, max_steps=10, debug=False):\n",
    "    \"\"\"Execute agentic loop: LLM calls tools until task is complete\"\"\"\n",
    "\n",
    "    # Display initial messages\n",
    "    print(\"\\n### Initial Messages:\")\n",
    "    for msg in pr:\n",
    "        role = msg['role']\n",
    "        content = msg.get('content', '[tool_call]')\n",
    "        print(f\"**{role}**: {content}\")\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        print(f\"\\n{'='*20} Step {step} {'='*20}\")\n",
    "\n",
    "        res = invoke(pr)\n",
    "\n",
    "        # Debug: show full response\n",
    "        if debug:\n",
    "            print(\"\\n**Debug - Full Response:**\")\n",
    "            pprint(res.choices[0].message.model_dump())\n",
    "\n",
    "        # Check if done\n",
    "        if not has_tools(res) or res.choices[0].finish_reason == 'stop':\n",
    "            print(\"\\nâœ“ Complete! Final response:\\n\")\n",
    "            content = res.choices[0].message.content\n",
    "            if content:\n",
    "                print(content)\n",
    "            return res\n",
    "\n",
    "        # Extract and execute tool\n",
    "        func = res2func_nm(res)\n",
    "        func_kwargs = get_res2func_kwargs(res)\n",
    "\n",
    "        print(f\"ğŸ”§ Tool: {func.__name__}\")\n",
    "        print(f\"   Args: {func_kwargs}\")\n",
    "\n",
    "        pr = next_msg(pr, res)\n",
    "\n",
    "        result_preview = pr[-1]['content'][:200]\n",
    "        print(f\"   Result: {result_preview}{'...' if len(pr[-1]['content']) > 200 else ''}\")\n",
    "\n",
    "    print(f\"\\nâš ï¸ Max steps ({max_steps}) reached!\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb3eed2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb3eed2b",
    "outputId": "e6500a30-cbb3-42a2-eb86-795b4897930c",
    "time_run": "8:04:49p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Initial Messages:\n",
      "**system**: You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.\n",
      "**user**: list all the files in the \"temp_dir\" directory.\n",
      "\n",
      "==================== Step 0 ====================\n",
      "\n",
      "**Debug - Full Response:**\n",
      "{'content': None,\n",
      " 'function_call': None,\n",
      " 'reasoning': \"User wants to list all files in temp_dir directory. We'll call \"\n",
      "              'list_directory.',\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"directory\":\"temp_dir\",\"long_format\":false,\"show_hidden\":false}',\n",
      "                              'name': 'list_directory'},\n",
      "                 'id': 'fc_f10ea2ae-a331-492e-a9df-b245850dffa4',\n",
      "                 'type': 'function'}]}\n",
      "ğŸ”§ Tool: list_directory\n",
      "   Args: {'directory': 'temp_dir', 'long_format': False, 'show_hidden': False}\n",
      "   Result: file1.py\n",
      "file2.py\n",
      "subdir\n",
      "test.ipynb\n",
      "\n",
      "==================== Step 1 ====================\n",
      "\n",
      "**Debug - Full Response:**\n",
      "{'content': '**Contents of `temp_dir`:**\\n'\n",
      "            '\\n'\n",
      "            '- `file1.py`  \\n'\n",
      "            '- `file2.py`  \\n'\n",
      "            '- `subdir`  \\n'\n",
      "            '- `test.ipynb`',\n",
      " 'function_call': None,\n",
      " 'reasoning': 'The user requested: \"list all the files in the \"temp_dir\" '\n",
      "              'directory.\" We have list_directory output: file1.py, file2.py, '\n",
      "              'subdir, test.ipynb. The output is correct. Should provide as '\n",
      "              'answer.',\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': None}\n",
      "\n",
      "âœ“ Complete! Final response:\n",
      "\n",
      "**Contents of `temp_dir`:**\n",
      "\n",
      "- `file1.py`  \n",
      "- `file2.py`  \n",
      "- `subdir`  \n",
      "- `test.ipynb`\n"
     ]
    }
   ],
   "source": [
    "pr = get_msg('list all the files in the \"temp_dir\" directory.')\n",
    "_ = tool_loop(pr, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "503df821",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "503df821",
    "outputId": "8a4cd484-a0c3-4ad4-90a9-09834b08da65",
    "time_run": "8:04:49p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Initial Messages:\n",
      "**system**: You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.\n",
      "**user**: Search for all .ipynb files in temp_dir, but only go 2 levels deep\n",
      "\n",
      "==================== Step 0 ====================\n",
      "ğŸ”§ Tool: find_files\n",
      "   Args: {'directory': 'temp_dir', 'file_type': 'f', 'maxdepth': 2, 'name': '*.ipynb'}\n",
      "   Result: temp_dir/subdir/another.ipynb\n",
      "temp_dir/test.ipynb\n",
      "\n",
      "==================== Step 1 ====================\n",
      "\n",
      "âœ“ Complete! Final response:\n",
      "\n",
      "Here are the `.ipynb` files found in `temp_dir` (search depth limited to 2 levels):\n",
      "\n",
      "- `temp_dir/test.ipynb`  \n",
      "- `temp_dir/subdir/another.ipynb`\n"
     ]
    }
   ],
   "source": [
    "pr = get_msg(\"Search for all .ipynb files in temp_dir, but only go 2 levels deep\")\n",
    "_ = tool_loop(pr, max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1805f506",
   "metadata": {
    "id": "1805f506",
    "time_run": "8:04:50p"
   },
   "outputs": [],
   "source": [
    "# Simple tests\n",
    "test_prompts = [\n",
    "    # Find files\n",
    "    \"Find all Python files in temp_dir\",\n",
    "    \"Search for all .ipynb files in temp_dir, but only go 2 levels deep\",\n",
    "    \"Find all directories in temp_dir\",\n",
    "\n",
    "    # Grep tests\n",
    "    \"Search for the word 'import' in the file temp_dir/file1.py\",\n",
    "    \"Find all lines containing 'litellm' in temp_dir/test.ipynb\",\n",
    "\n",
    "    # Multi-tool tests\n",
    "    \"Find all Python files in temp_dir and then search for 'def safe_run' in them\",\n",
    "    \"Look for all .py files in temp_dir and tell me which ones contain the word 'fastcore'\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54645673",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54645673",
    "outputId": "28b60fe5-9110-4326-b93a-50ed939b3592",
    "time_run": "8:04:50p",
    "use_thinking": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Find all Python files in temp_dir',\n",
       " \"Search for the word 'import' in the file temp_dir/file1.py\",\n",
       " \"Look for all .py files in temp_dir and tell me which ones contain the word 'fastcore'\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts[::3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X3Q3-C0VewQu",
   "metadata": {
    "id": "X3Q3-C0VewQu"
   },
   "source": [
    "I am just calling the tool for 3 prompts. As I am using free tier for service. It might fail with the rate limiting error, please wait and retry.\n",
    "The llm might give wrong ans like not searching a directory(due to the non deterministic nature of the model inference). There is an awsome blog from [Thinking Machine Lab](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) for this specific behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e35010f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e35010f",
    "outputId": "10d42f58-99d9-44ff-a9eb-ec5023110b3e",
    "time_run": "8:04:54p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Initial Messages:\n",
      "**system**: You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.\n",
      "**user**: Find all Python files in temp_dir\n",
      "\n",
      "==================== Step 0 ====================\n",
      "ğŸ”§ Tool: find_files\n",
      "   Args: {'directory': 'temp_dir', 'file_type': 'f', 'name': '*.py'}\n",
      "   Result: temp_dir/file2.py\n",
      "temp_dir/file1.py\n",
      "temp_dir/subdir/file3.py\n",
      "\n",
      "==================== Step 1 ====================\n",
      "\n",
      "âœ“ Complete! Final response:\n",
      "\n",
      "Here are all the Python files located under `temp_dir`:\n",
      "\n",
      "```\n",
      "temp_dir/file2.py\n",
      "temp_dir/file1.py\n",
      "temp_dir/subdir/file3.py\n",
      "```\n",
      "\n",
      "Let me know if youâ€™d like to inspect any of these files or search inside them!\n",
      "'*********************************************************************************************************'\n",
      "\n",
      "### Initial Messages:\n",
      "**system**: You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.\n",
      "**user**: Search for the word 'import' in the file temp_dir/file1.py\n",
      "\n",
      "==================== Step 0 ====================\n",
      "ğŸ”§ Tool: grep_files\n",
      "   Args: {'file_path': 'temp_dir/file1.py', 'ignore_case': False, 'line_numbers': False, 'pattern': 'import', 'show_filename': True}\n",
      "   Result: temp_dir/file1.py:import numpy as np\n",
      "temp_dir/file1.py:from fastcore.xtras import run\n",
      "\n",
      "==================== Step 1 ====================\n",
      "\n",
      "âœ“ Complete! Final response:\n",
      "\n",
      "**Search results for the word â€œimportâ€ in `temp_dir/file1.py`:**\n",
      "\n",
      "```\n",
      "temp_dir/file1.py:import numpy as np\n",
      "temp_dir/file1.py:from fastcore.xtras import run\n",
      "```\n",
      "\n",
      "These are the only lines in the file that contain the word `import`.\n",
      "'*********************************************************************************************************'\n",
      "\n",
      "### Initial Messages:\n",
      "**system**: You help users find and search files safely using command-line tools. Use find_files for locating files, grep_files for searching content, and list_directory for browsing. Provide clear, accurate results based only on tool outputs.\n",
      "**user**: Look for all .py files in temp_dir and tell me which ones contain the word 'fastcore'\n",
      "\n",
      "==================== Step 0 ====================\n",
      "ğŸ”§ Tool: find_files\n",
      "   Args: {'directory': 'temp_dir', 'file_type': '', 'maxdepth': -1, 'name': '*.py'}\n",
      "   Result: temp_dir/file2.py\n",
      "temp_dir/file1.py\n",
      "temp_dir/subdir/file3.py\n",
      "\n",
      "==================== Step 1 ====================\n",
      "ğŸ”§ Tool: grep_files\n",
      "   Args: {'file_path': 'temp_dir/file1.py', 'ignore_case': False, 'line_numbers': False, 'pattern': 'fastcore', 'show_filename': True}\n",
      "   Result: temp_dir/file1.py:from fastcore.xtras import run\n",
      "\n",
      "==================== Step 2 ====================\n",
      "ğŸ”§ Tool: grep_files\n",
      "   Args: {'file_path': 'temp_dir/file2.py', 'ignore_case': False, 'line_numbers': False, 'pattern': 'fastcore', 'show_filename': True}\n",
      "   Result: \n",
      "\n",
      "==================== Step 3 ====================\n",
      "ğŸ”§ Tool: grep_files\n",
      "   Args: {'file_path': 'temp_dir/subdir/file3.py', 'ignore_case': False, 'line_numbers': False, 'pattern': 'fastcore', 'show_filename': True}\n",
      "   Result: temp_dir/subdir/file3.py:from fastcore.script import call_parse\n",
      "\n",
      "==================== Step 4 ====================\n",
      "\n",
      "âœ“ Complete! Final response:\n",
      "\n",
      "Here are the `.py` files in `temp_dir` that contain the word **`fastcore`**:\n",
      "\n",
      "| File | Line(s) that match |\n",
      "|------|--------------------|\n",
      "| `temp_dir/file1.py` | `from fastcore.xtras import run` |\n",
      "| `temp_dir/subdir/file3.py` | `from fastcore.script import call_parse` |\n",
      "\n",
      "`temp_dir/file2.py` does **not** contain the word `fastcore`.\n",
      "'*********************************************************************************************************'\n"
     ]
    }
   ],
   "source": [
    "for i in test_prompts[::3]:\n",
    "    pr = get_msg(i)\n",
    "    _ = tool_loop(pr, max_steps=10)\n",
    "    pprint(\"***\" * 35 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73bb2b",
   "metadata": {
    "id": "2b73bb2b"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We've built a complete file search agent from scratch, demystifying how LLM agents actually work under the hood. Starting with simple message structures, we progressed through safe tool design, schema generation, and finally implemented a full agent loop that reasons and acts.\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "- **Agents = LLM reasoning + tool execution** - The model decides *what* to do, your code provides the *how*\n",
    "- **Safety first** - Whitelisting commands and validating inputs is crucial when executing system commands\n",
    "- **The agent loop is simple** - It's just: call LLM â†’ execute tools â†’ feed back results â†’ repeat until done\n",
    "- **Tool schemas bridge the gap** - They teach the LLM what capabilities it has available\n",
    "\n",
    "This foundation extends far beyond file search. The same patterns work for:\n",
    "- Database agents that query and analyze data\n",
    "- Web automation agents using Playwright or Selenium  \n",
    "- API integration agents that coordinate multiple services\n",
    "- Code execution agents for data science workflows and many more\n",
    "\n",
    "By building from first principle, we understand exactly what's happening at each step, making debugging easier and giving us full control over safety, costs, and behavior.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
