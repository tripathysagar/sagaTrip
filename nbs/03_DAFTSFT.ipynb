{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaption Fine-Tuning with LoRA: My Experiment on Mac M1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim** of the experiment is to implement train a small LLM on new domain and prime it for that domain specific sythetic data.\n",
    "\n",
    "LLM training haapens in two steps:\n",
    "1. base model : trained on raw text for next token prediction(classic language modeling). Which is done by all language model by training on all the crawed web data.\n",
    "1. instruct model : Train the base model on the further curated data where real magic happens. Like follwing instruction, multilayer chat, to make it helpful, safe, and aligned with user expectations etc . This process is known as RLHF.\n",
    "\n",
    "Below is a basic pipeline for RLHF, taken from [this excellent blog by Chip Huyen](https://huyenchip.com/2023/05/02/rlhf.html)\n",
    "\n",
    "<img title=\"a title\" alt=\"Alt text\" src=\"./static/blog3/RLHF.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "I work at Amdocs in the telecom domain, where a lot of knowledge is stored in Confluence pages. I wanted to explore whether a small model fine-tuned on a subset of our internal wiki could assist with internal Q&A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup & Constraints\n",
    "1. **Hardware**: Mac M1 (no NVIDIA GPU, so training is on CPU/MPS backend ‚Äî slower than CUDA).\n",
    "1. **Data**: Small subset of internal wiki pages (only public-safe subset, for experimentation).\n",
    "1. **Compute**: Due to hardware constraints, we trained on a very small sample to validate pipeline, not to reach SOTA results.\n",
    "1. **Goal**:\n",
    "    - Validate that DoRA + LoRA SFT works end-to-end.\n",
    "    - Measure how much domain knowledge the model can absorb with few steps.\n",
    "1. **Code**: All the code for the experiment is present in the [qa_sys](https://github.com/tripathysagar/qa_sys.git). Follow the notebook only no propritary data is shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "### üï∑Ô∏è Data scraping from confluence\n",
    "\n",
    "The data is stored in org's private server. I extracted all the pages and all its child wikis using the awesome lib `atlassian-python-api`. It provides which provides a simple way to connect to Confluence using access token and conflunece url. \n",
    "\n",
    "Then, I wrote a small recursive crawler (BFS style) to:\n",
    "1. Fetch a page‚Äôs content.\n",
    "1. Retrieve its child pages.\n",
    "1. Repeat until the entire hierarchy is traversed.\n",
    "\n",
    "For each page I used `html2text` lib for converting a simple webpage to markdown dump as well as few clean ups like comments attributes for simplicity. Fetched around 4654 pages.\n",
    "\n",
    "This gives a structural raw text to be used for training base model.\n",
    "\n",
    "### üèóÔ∏è Building Synthetic Data\n",
    "\n",
    "1. **`Synthetic Data Kit`** : I used Meta‚Äôs synthetic-data kit to generate question‚Äìanswer pairs from each page. \n",
    "1. **Role-based Diversity** : To make the dataset more robust, I instructed the model to generate Q&A pairs from the perspective of four roles:\n",
    "    - **BA (Business Analyst)** ‚Äì business rules, compliance, ROI.\n",
    "    - **SA (System Analyst)** ‚Äì workflows, dependencies, data flows.\n",
    "    - **DEV (Developer)** ‚Äì API inputs/outputs, error handling, edge cases.\n",
    "    - **QA (Tester)** ‚Äì test cases, edge cases, validation.\n",
    "1. The SDK was configured to use `openai/gpt-oss-20b`, running locally via `LM Studio` on my Mac M1.\n",
    "1. **Focus on Speed** :\n",
    "    - I only ran the create step of the SDK (no curation) to minimize generation time on Mac hardware.\n",
    "    - This meant I directly collected the generated Q&A without additional filtering.\n",
    "1. **Prompt** :\n",
    "    Below is the exact prompt I used to generate Q&A pairs using SDK:\n",
    "    ```yml\n",
    "      qa_generation: | \n",
    "      You are a synthetic data generator for API and business process documentation. The input is a document describing one or more processes, APIs, or business requirements.\n",
    "\n",
    "      **Instructions:**\n",
    "      1. Automatically identify the **document title** from the content.  \n",
    "        - Use the inferred title **naturally** in every question and answer.  \n",
    "        - If no clear title exists, you may use the filename without extension as the title.  \n",
    "      2. Generate question-and-answer pairs for the following roles:  \n",
    "        - **Business Analyst (BA):** focus on requirements, stakeholder value, business rules, process optimization, compliance, risk, and ROI.  \n",
    "        - **System Analyst / Software Analyst (SA):** focus on system interactions, workflow, dependencies, data flows, and integration points.  \n",
    "        - **Developer (DEV):** focus on inputs, outputs, API parameters, request/response examples, error handling, and implementation considerations.  \n",
    "        - **QA / Tester (QA):** focus on test cases, edge cases, validation, error scenarios, and business rule verification.\n",
    "      3. Each question and answer must **refer to the inferred title** naturally. Example:\n",
    "        - Question: \"For the `User Management process`, what are the steps and required inputs?\"\n",
    "        - Answer: \"The `User Management process` requires two APIs: `GET token` for authentication and `PUT ManageUser` for updating user details.\"\n",
    "      4. Only generate Q&A from the provided content; do not invent information.\n",
    "      5. If the document contains only links or very little meaningful content, output an empty array `[]`.\n",
    "      6. Ensure questions and answers are clear, self-contained, and unambiguous.\n",
    "      7. Avoid duplicate questions.\n",
    "      8. Generate at least **one Q&A per role** if information allows.\n",
    "      9. Each question and answer must integrate the inferred title naturally.\n",
    "      10. Do not infer or invent details not present in the document. If unclear, omit the Q&A for that role.\n",
    "      11. If the document is processed in chunks, ensure Q&A is relevant only to the current chunk.\n",
    "      12. Extract all high qualities Q&A pairs possible, up to the 5 per role.\n",
    "      13. Think hard before generation\n",
    "\n",
    "        **Output format:**\n",
    "          [\n",
    "            {{\n",
    "              \"title\": \"inferred_document_title\",\n",
    "              \"role\": \"BA\",\n",
    "              \"question\": \"...\",\n",
    "              \"answer\": \"...\"\n",
    "            }},\n",
    "            {{\n",
    "              \"title\": \"inferred_document_title\",\n",
    "              \"role\": \"SA\",\n",
    "              \"question\": \"...\",\n",
    "              \"answer\": \"...\"\n",
    "            }},\n",
    "            {{\n",
    "              \"title\": \"inferred_document_title\",\n",
    "              \"role\": \"DEV\",\n",
    "              \"question\": \"...\",\n",
    "              \"answer\": \"...\"\n",
    "            }},\n",
    "            {{\n",
    "              \"title\": \"inferred_document_title\",\n",
    "              \"role\": \"QA\",\n",
    "              \"question\": \"...\",\n",
    "              \"answer\": \"...\"\n",
    "            }}\n",
    "          ]\n",
    "          Text:\n",
    "          {text}\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "  1. **Execution time**: The complete generation took around couple of days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Domain adaption LoRA of base model\n",
    "\n",
    "1. **Model Choice**:\n",
    "    - For speed and performance on my Mac M1, I used `google/gemma-3-270m`.\n",
    "    - Downloaded using Hugging Face transformers (both model & tokenizer).\n",
    "\n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/download_model.png\" width=600>\n",
    "\n",
    "1. **Context Window**:\n",
    "    - Used a context length of 512 tokens with an overlap of 128 tokens to preserve context across chunks.\n",
    "    - Total tokens to be processed: 4,302,414.\n",
    "\n",
    "1. **LoRA Configuration**:\n",
    "    - Targeted only the attention block‚Äôs linear modules: q_proj, k_proj, v_proj.\n",
    "    - Used rank = 16, lora_alpha = 32 for a good balance of capacity and speed.\n",
    "    - bias was not selected and drop out is set to 0.1\n",
    "\n",
    "1. **Train/Validation Split**:\n",
    "    - Data was split 95:5 into train and validation sets.\n",
    "\n",
    "1. **Baseline Perplexity**:\n",
    "    - It is measured how well a language model predicts the next token in a seqence\n",
    "    - Calculated by finding conditional probabilty of next token wrt past tokens \n",
    "    - Lower perplexity indicates the model has learned the input distribution better ‚Äî it is ‚Äúless surprised‚Äù by the text.\n",
    "    - On-domain (wiki text): 35.87\n",
    "    - Out-of-domain (Wikipedia text): 52.28\n",
    "    - Out-of-domain evaluation was used to check for catastrophic forgetting during fine-tuning.\n",
    "\n",
    "1. **Training hyperparams**:\n",
    "    - as all the inputs are of shpae 512 \n",
    "    - below are the params\n",
    "        ```py\n",
    "        batch_size = 4\n",
    "        gradient_accumulation_steps = 2 # keeping it low for not overflowing \n",
    "        learning_rate=1e-4              # an baseline learning_rate suggested by lora paper\n",
    "        num_train_epochs=3              \n",
    "        ```\n",
    "    - other hyper params are as follows:\n",
    "\n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/base_hyperprams.png\" width=800>\n",
    "1. **Loss landsscape**:\n",
    "    - used `trackio` for tracking as keeping in the spirit of running in local\n",
    "    - The loss graph is gradual as below.\n",
    "        <div style=\"display: flex; justify-content: space-between; gap: 5px;\">\n",
    "            <div style=\"flex: 1; text-align: center;\">\n",
    "                <img src=\"./static/blog3/base_train_loss.png\" alt=\"Training Loss\" width=\"600\">\n",
    "                <p>Training Loss</p>\n",
    "            </div>\n",
    "            <div style=\"flex: 1; text-align: center;\">\n",
    "                <img src=\"./static/blog3/base_eval_loss.png\" alt=\"Evaluation Loss\" width=\"600\">\n",
    "                <p>Evaluation Loss</p>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "\n",
    "1. **Results After Fine-Tuning**:\n",
    "    - On-domain perplexity improved to 4.19 üéâ\n",
    "    - Out-of-domain perplexity improved slightly to 40.64 (no significant forgetting).\n",
    "\n",
    "1. **Saving model**:\n",
    "    - Combine the LoRA adapters with the base model to produce a single, unified model.\n",
    "    - Save the model and tokenizer for further processing\n",
    "    \n",
    "1. **Vibe check**:\n",
    "    - the trained model is learning from the business wiki\n",
    "    - below is the inference \n",
    "\n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/base_model_generate.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Further SFT on base model\n",
    "1. **Message Templete**:\n",
    "    For each question and answer pairs below is the input text.\n",
    "    ```py\n",
    "    SYSTEM_PROMPT = 'You are a senior software developer. Answer truthfully and concisely.\\nIf unsure, reply \"I do not know.\" Explain steps briefly when needed.'\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "1. **Context Window**:\n",
    "    - Aim to find an appropriate context length is key. Too short the model misses to learn, too long adds unnecessary padding, wasting memory and computation. \n",
    "    - As a rule of thumb selecting context window of 512 is idle for the formatted text. As it a whole no of power 2 to make GPU computational efficiently.\n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/SFT_context_size.png\" width=800>\n",
    "\n",
    "\n",
    "1. **LoRA Configuration**:\n",
    "    - For simplicity choosing same LoRA as base model.\n",
    "\n",
    "1. **Train/Validation Split**:\n",
    "    - Data was split 95:0.4:5 into train:valid:test sets. Smaller valid set for faster training.\n",
    "    - Test set is not used during training\n",
    "\n",
    "1. **Baseline Entropy and Mean Token Accuracy**:\n",
    "    - Entropy measures the model‚Äôs uncertainty ‚Äî lower entropy after SFT means the model is more confident about predicting the next token.\n",
    "    - Mean Token Accuracy tracks how often the model predicts the correct next token, and should increase after SFT, showing better alignment with domain data.\n",
    "    - HF [wiki](https://huggingface.co/docs/trl/en/sft_trainer#logged-metrics) refenece\n",
    "        \n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/SFT_before_metrics.png\" width=800>\n",
    "\n",
    "1. **Training hyperparams**:\n",
    "    - below are the params\n",
    "        ```py\n",
    "        batch_size = 8                              # for faster training and many input are below 100 tokens\n",
    "        gradient_accumulation_steps = 4             \n",
    "        learning_rate=1e-4                          # stable learning rate\n",
    "        max_length=512\n",
    "        num_train_epochs=1                          # increaing the epochs more than 1 leads to explosion of loss, and over flowing of loss\n",
    "        logging_steps, eval_steps, save_steps = 50, 50, 50    \n",
    "        ```\n",
    "    - other hyper params are as follows:\n",
    "\n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/SFT_traing_args.png\" width=800>\n",
    "\n",
    "1. **Loss landsscape**:\n",
    "    - The loss graph is gradual as below.\n",
    "        <div style=\"display: flex; justify-content: space-between; gap: 5px;\">\n",
    "            <div style=\"flex: 1; text-align: center;\">\n",
    "                <img src=\"./static/blog3/SFT_train_loss.png\" alt=\"Training Loss\" width=\"600\">\n",
    "                <p>Training Loss</p>\n",
    "            </div>\n",
    "            <div style=\"flex: 1; text-align: center;\">\n",
    "                <img src=\"./static/blog3/SFT_eval_loss.png\" alt=\"Evaluation Loss\" width=\"600\">\n",
    "                <p>Evaluation Loss</p>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "\n",
    "1. **Results After Fine-Tuning**:\n",
    "    - After training the Entropy decresed to 2.44 and Mean Token Accuracy is improved to 57% üéâ\n",
    "\n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/SFT_after_metrics.png\" width=800>\n",
    "\n",
    "1. **Saving model**:\n",
    "    - followed same approcah as base model\n",
    "    \n",
    "1. **Vibe check**:\n",
    "    - the trained model is learning from the business wiki\n",
    "    - below is the a sample QA\n",
    "\n",
    "        <img title=\"Downloading the base model\" alt=\"Downloading the base model from Hugging Face\" src=\"./static/blog3/SFT_vibe_check.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this experiment, I built a complete pipeline: scraping domain data from Confluence, generating synthetic Q&A data, and performing continual pretraining followed by SFT. The model showed decent results but still hallucinates and sometimes produces irrelevant text ‚Äî a clear signal that further refinement is needed. No efficent packing and speed up is not able to achived as training on Mac is painfully slow wrt Nvidia gpu.\n",
    "\n",
    "**Key takeaways and next steps**:\n",
    "\n",
    "1. **Improve data diversity**: As data is generated by a LLM, the performance will improve after adding more sythtic data and some real world opensource data for generalization.\n",
    "1. **Use multiple LLMs for generation**: Include multiple LLM for synthetic data generation which will give raises to more entropy of information\n",
    "1. **Experiment with larger base models**: Using bigger model for having grater knowledge absorption\n",
    "1. **Tune LoRA configs and hyperparameters**: Having differenet configs for LoRA ranks and other hyper params in base model wihich will lead to storong adaption of source raw text.\n",
    "1. **Building scheduler for SFT**: The SFT triner fails dramtically when running with a decent lower lr i.e. 5e-5. To faster convergenece we have to use higher learning rate so that the model will not be stuck around local minima or saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
